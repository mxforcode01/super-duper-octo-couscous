EMB_MODEL = SentenceTransformer(embedding_path)  # single source of truth

def encode_text(text: str, normalize: bool = False):
    v = EMB_MODEL.encode(text)
    if normalize:
        # cosine is scale-invariant if both sides normalized; keep it consistent
        import numpy as np
        n = np.linalg.norm(v)
        if n > 0:
            v = v / n
    return v

def make_query_text(q: str) -> str:
    return "query: " + str(q)  # matches your Qdrant behavior

def dump_into_lancedb(text_chunks, db_path, table_name):
    db = lancedb.connect(db_path)
    if table_name in db.table_names():
        db.drop_table(table_name)

    rows = []
    for i, ch in enumerate(text_chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            "vector": encode_text(ch["text"], normalize=True),  # normalize on write
            **ch["metadata"]
        })
    df = pd.DataFrame(rows)
    tbl = db.create_table(table_name, df)

    # Build an ANN index with cosine metric.
    # If your dataset is small (<~100k), IVF_PQ is still fine; bump recall with nprobes later.
    # If you prefer exact baseline first, comment this out and rely on flat scan while testing.
    tbl.create_index(
        column="vector",
        index_type="ivf_pq",          # or "ivf_flat" / "hnsw" if available in your version
        metric="cosine",
        num_partitions=64,            # tune for your data size (e.g., ~sqrt(N))
        num_sub_vectors=16            # PQ granularity; raise for better recall
    )

def lancedb_where_clause(filter_dict: dict | None) -> str | None:
    if not filter_dict:
        return None
    parts = []
    for k, v in filter_dict.items():
        if v is None:
            continue
        # allow list-like values
        if isinstance(v, (list, tuple, set)):
            vv = [f"'{str(x).replace(\"'\", \"''\")}'" for x in v]
            parts.append(f"{k} IN ({', '.join(vv)})")
        else:
            s = str(v).replace("'", "''")
            parts.append(f"{k} = '{s}'")
    return " AND ".join(parts) if parts else None

def search_lancedb(query: str, table, top_k: int = 20, filter_dict: dict | None = None):
    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True)

    # Build search
    sq = table.search(qvec).metric("cosine")

    # Prefilter BEFORE the ANN probe (closer to Qdrantâ€™s integrated filter)
    where = lancedb_where_clause(filter_dict)
    if where:
        sq = sq.where(where)  # LanceDB treats this as prefilter for the vector scan

    # Overfetch similar to your Qdrant loop and refine
    over_k = max(top_k * 3, top_k + 10)   # extra headroom
    sq = sq.limit(over_k).refine_factor(3)   # re-rank a larger candidate set exactly

    # Probe more partitions (higher recall; slower). Tune 8â€“32.
    sq = sq.nprobes(16)

    df = sq.to_pandas()

    # Clip to final top_k (already ranked by distance)
    return df.head(top_k)

###########################################################
# Qdrant
from qdrant_client.http.models import SearchParams

def dump_into_qdrant(chunks, collection_name="my_docs_bge"):
    client = QdrantClient(path=qdrant_path)
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=768, distance=Distance.COSINE)
    )

    rows = []
    for i, ch in enumerate(chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            **ch["metadata"]
        })

    texts = [r["text"] for r in rows]
    payloads = rows
    vectors = [encode_text(t, normalize=True).tolist() for t in texts]  # normalize on write (optional but consistent)

    ids = [r["id"] for r in rows]

    client.upload_collection(
        collection_name=collection_name,
        vectors=vectors,
        payload=payloads,
        ids=ids,
    )
    return

def search_qdrant(query, client, collection_name="my_docs_bge", top_k=20, stages: list[dict] | None=None):
    if stages is None:
        stages = [{}]

    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True).tolist()

    results = []
    seen_ids = set()
    for stage in stages:
        need = top_k - len(results)
        if need <= 0:
            break
        flt = make_filter(stage)
        hits = client.search(
            collection_name=collection_name,
            query_vector=qvec,
            query_filter=flt,
            limit=max(need*3, need+10),
            with_payload=True,
            params=SearchParams(hnsw_ef=128, exact=False)  # make â€œdefaultâ€ recall explicit
        )
        for pt in hits:
            if pt.id not in seen_ids:
                results.append(pt)
                seen_ids.add(pt.id)
                if len(results) >= top_k:
                    break
    return results

##############################################################################################

import numpy as np

from typing import Any, Dict, Optional

def _is_all(v: Any) -> bool:
    return isinstance(v, str) and v.strip().upper() == "ALL"

def _sql_quote(v: Any) -> str:
    # Quote strings safely (double single quotes); pass numbers/bools through.
    if isinstance(v, str):
        return "'" + v.replace("'", "''") + "'"
    if isinstance(v, bool):
        return "TRUE" if v else "FALSE"
    return str(v)

def build_filter_string(filters: Dict[str, Any]) -> Optional[str]:
    """
    Build a LanceDB SQL-like WHERE clause from dict values.
    Supports scalars and sequences. If any value in a key's list is 'ALL',
    that key is skipped (no filtering on that field).
    Examples:
      {'country': ['SINGAPORE','MALAYSIA'], 'product':['CASH']}
         -> "country IN ('SINGAPORE','MALAYSIA') AND product = 'CASH'"
    """
    if not filters:
        return None

    clauses = []
    for key, raw in filters.items():
        if raw is None:
            continue

        # Normalize to a list for uniform handling
        if isinstance(raw, (list, tuple, set)):
            values = [v for v in raw if v is not None and (not isinstance(v, str) or v.strip() != "")]
        else:
            values = [raw]

        if not values:
            continue

        # If ANY value is 'ALL', skip this key entirely (no filter on it)
        if any(_is_all(v) for v in values):
            continue

        # De-dupe while preserving order (case-insensitive for strings)
        seen = set()
        deduped = []
        for v in values:
            key_v = v.upper() if isinstance(v, str) else v
            if key_v in seen:
                continue
            seen.add(key_v)
            deduped.append(v)

        # Emit = or IN (...)
        if len(deduped) == 1:
            clauses.append(f"{key} = {_sql_quote(deduped[0])}")
        else:
            joined = ", ".join(_sql_quote(v) for v in deduped)
            clauses.append(f"{key} IN ({joined})")

    return " AND ".join(clauses) if clauses else None


def execute_search(
    table,
    model,
    query_text: str,
    filters: Dict[str, object],
    limit: int = 20,
    overfetch_mult: int = 3,
    refine_factor: int = 3,
    nprobes: int = 16,
) -> Tuple[pd.DataFrame, int]:
    """
    Qdrant-like LanceDB search:
    - cosine metric
    - normalized query vector
    - prefilter via .where(...)
    - overfetch + refine + nprobes to raise recall
    """
    qtxt = make_query_text(query_text)
    qvec = encode_query(model, qtxt, normalize=True)

    # Drop any 'ALL' values to match your usual semantics
    filters = drop_all_values(filters)
    fstr = build_filter_string(filters)

    # Overfetch similar to your Qdrant loop (need * 3)
    over_k = max(limit * overfetch_mult, limit + 10)

    sq = table.search(qvec).metric("cosine")
    if fstr:
        sq = sq.where(fstr)           # PREFILTER: affect the candidate set
    sq = sq.nprobes(nprobes)          # probe more partitions (higher recall)
    sq = sq.refine_factor(refine_factor)  # re-rank a larger candidate set exactly
    sq = sq.limit(over_k)             # over-fetch then clip

    df = sq.to_pandas()
    # NOTE: LanceDB returns already sorted results (by distance/score).
    # Clip to requested limit:
    df = df.head(limit)
    return df, len(df)

FILTER_PRIORITY_DEFAULT = {
    "topic": 1,
    "product": 2,
    "client_type": 3,
    "region": 4,
    "country": 5,
}

def clean_filters(filters: Dict[str, str]) -> Dict[str, str]:
    return {k: v for k, v in (filters or {}).items() if v is not None and str(v) != ""}

def order_filters_by_priority(
    active_filters: Dict[str, str],
    priority_map: Dict[str, int] = FILTER_PRIORITY_DEFAULT
) -> List[Tuple[str, str]]:
    return sorted(active_filters.items(), key=lambda kv: priority_map.get(kv[0], 999))

def progressive_search(
    table,
    model,
    query_text: str,
    initial_filters: Dict[str, str],
    max_results: int = 20,
    filter_priority: Dict[str, int] = FILTER_PRIORITY_DEFAULT,
    verbose: bool = True
) -> pd.DataFrame:
    active_filters = drop_all_values(clean_filters(initial_filters.copy()))
    iterations = 0

    # 1) Try with all filters
    df, count = execute_search(table, model, query_text, active_filters, limit=max_results)
    iterations += 1
    if count >= max_results or (count > 0 and len(active_filters) == 0):
        return df

    # 2) Relax in priority order (topic â†’ product â†’ client_type â†’ region â†’ country)
    for key, _ in order_filters_by_priority(active_filters, filter_priority):
        active_filters.pop(key, None)
        iterations += 1
        df, count = execute_search(table, model, query_text, active_filters, limit=max_results)
        if count >= max_results or not active_filters:
            break

    return df

################################################################################
# Claude 
"""
LanceDB Advanced Progressive Filter System
Integrates multi-value filtering, "ALL" handling, and optimized search
"""

import lancedb
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import Any, Dict, Optional, List, Tuple
import time

# Initialize model
model = SentenceTransformer('all-MiniLM-L6-v2')

# ========================================
# CORE FILTERING UTILITIES
# ========================================

def _is_all(v: Any) -> bool:
    """Check if a value represents 'ALL' (no filtering)"""
    return isinstance(v, str) and v.strip().upper() == "ALL"

def _sql_quote(v: Any) -> str:
    """Quote values safely for SQL-like queries"""
    if isinstance(v, str):
        return "'" + v.replace("'", "''") + "'"
    if isinstance(v, bool):
        return "TRUE" if v else "FALSE"
    return str(v)

def build_filter_string(filters: Dict[str, Any]) -> Optional[str]:
    """
    Build a LanceDB SQL-like WHERE clause from dict values.
    Supports scalars and sequences. If any value in a key's list is 'ALL',
    that key is skipped (no filtering on that field).
    
    Examples:
      {'country': ['SINGAPORE','MALAYSIA'], 'product': 'CASH'}
         -> "country IN ('SINGAPORE','MALAYSIA') AND product = 'CASH'"
      {'country': 'ALL', 'product': 'CASH'}
         -> "product = 'CASH'" (country is skipped)
    """
    if not filters:
        return None

    clauses = []
    for key, raw in filters.items():
        if raw is None:
            continue

        # Normalize to a list for uniform handling
        if isinstance(raw, (list, tuple, set)):
            values = [v for v in raw if v is not None and (not isinstance(v, str) or v.strip() != "")]
        else:
            values = [raw] if raw != "" else []

        if not values:
            continue

        # If ANY value is 'ALL', skip this key entirely (no filter on it)
        if any(_is_all(v) for v in values):
            continue

        # De-duplicate while preserving order (case-insensitive for strings)
        seen = set()
        deduped = []
        for v in values:
            key_v = v.upper() if isinstance(v, str) else v
            if key_v in seen:
                continue
            seen.add(key_v)
            deduped.append(v)

        # Emit = or IN (...)
        if len(deduped) == 1:
            clauses.append(f"{key} = {_sql_quote(deduped[0])}")
        else:
            joined = ", ".join(_sql_quote(v) for v in deduped)
            clauses.append(f"{key} IN ({joined})")

    return " AND ".join(clauses) if clauses else None

def drop_all_values(filters: Dict[str, Any]) -> Dict[str, Any]:
    """Remove any filter keys that have 'ALL' as their value"""
    if not filters:
        return {}
    
    cleaned = {}
    for key, value in filters.items():
        # Check if it's 'ALL' (single value or in a list)
        if isinstance(value, (list, tuple, set)):
            non_all_values = [v for v in value if not _is_all(v)]
            if non_all_values:  # Only keep if there are non-ALL values
                cleaned[key] = non_all_values if len(non_all_values) > 1 else non_all_values[0]
        elif not _is_all(value):
            cleaned[key] = value
    
    return cleaned

def clean_filters(filters: Dict[str, Any]) -> Dict[str, Any]:
    """Clean filters by removing None and empty string values"""
    return {k: v for k, v in (filters or {}).items() 
            if v is not None and (not isinstance(v, str) or v.strip() != "")}

def encode_query(model, query_text: str, normalize: bool = True) -> np.ndarray:
    """Encode query text to vector, optionally normalize for cosine similarity"""
    vec = model.encode(query_text)
    if normalize:
        vec = vec / np.linalg.norm(vec)
    return vec

# ========================================
# SEARCH EXECUTION
# ========================================

def execute_search(
    table,
    model,
    query_text: str,
    filters: Dict[str, Any],
    limit: int = 20,
    overfetch_mult: int = 3,
    refine_factor: int = 3,
    nprobes: int = 20,
) -> Tuple[pd.DataFrame, int]:
    """
    Optimized LanceDB search with:
    - Cosine similarity metric
    - Normalized query vector
    - Pre-filtering via where clause
    - Overfetch + refine + nprobes for higher recall
    
    Args:
        table: LanceDB table
        model: Sentence transformer model
        query_text: Search query
        filters: Filter dictionary (supports lists and 'ALL' values)
        limit: Maximum results to return
        overfetch_mult: Multiplier for initial candidate retrieval
        refine_factor: Re-ranking factor for accuracy
        nprobes: Number of partitions to search (higher = better recall)
    """
    # Encode and normalize query
    qvec = encode_query(model, query_text, normalize=True)
    
    # Clean filters and build SQL string
    filters = drop_all_values(filters)
    filter_string = build_filter_string(filters)
    
    # Overfetch to ensure we get enough results after filtering
    over_k = max(limit * overfetch_mult, limit + 10)
    
    # Build search query
    search_query = table.search(qvec).metric("cosine")
    
    if filter_string:
        search_query = search_query.where(filter_string)
    
    search_query = (search_query
                   .nprobes(nprobes)           # Probe more partitions
                   .refine_factor(refine_factor)  # Re-rank candidates
                   .limit(over_k))                # Over-fetch
    
    # Execute search
    df = search_query.to_pandas()
    
    # Clip to requested limit
    df = df.head(limit)
    return df, len(df)

# ========================================
# PROGRESSIVE FILTER SYSTEM
# ========================================

# Default filter priority (lower number = removed first)
FILTER_PRIORITY_DEFAULT = {
    "topic": 1,        # Removed first
    "product": 2,
    "client_type": 3,
    "region": 4,
    "country": 5,      # Removed last (most important)
}

def order_filters_by_priority(
    active_filters: Dict[str, Any],
    priority_map: Dict[str, int] = None
) -> List[Tuple[str, Any]]:
    """Order filters by priority for progressive relaxation"""
    if priority_map is None:
        priority_map = FILTER_PRIORITY_DEFAULT
    return sorted(active_filters.items(), 
                 key=lambda kv: priority_map.get(kv[0], 999))

def progressive_search(
    table,
    model,
    query_text: str,
    initial_filters: Dict[str, Any],
    max_results: int = 20,
    filter_priority: Dict[str, int] = None,
    verbose: bool = True,
    overfetch_mult: int = 3,
    refine_factor: int = 3,
    nprobes: int = 20,
) -> Dict:
    """
    Progressive filter relaxation search with advanced filtering support.
    
    Supports:
    - Multi-value filters: {'country': ['SINGAPORE', 'MALAYSIA']}
    - 'ALL' values that disable filtering: {'country': 'ALL'}
    - Progressive relaxation based on priority
    - Optimized search with overfetch and refinement
    
    Args:
        table: LanceDB table
        model: Sentence transformer model
        query_text: Search query
        initial_filters: Initial filter dictionary
        max_results: Target number of results
        filter_priority: Custom priority map (optional)
        verbose: Print detailed progress
        overfetch_mult: Overfetch multiplier for better recall
        refine_factor: Refinement factor for re-ranking
        nprobes: Number of partitions to probe
        
    Returns:
        Dictionary with results and metadata
    """
    start_time = time.time()
    
    if filter_priority is None:
        filter_priority = FILTER_PRIORITY_DEFAULT
    
    # Clean and prepare filters
    active_filters = drop_all_values(clean_filters(initial_filters.copy()))
    removed_filters = []
    iterations = 0
    search_history = []
    
    if verbose:
        print("\n" + "="*60)
        print("ðŸ” PROGRESSIVE FILTER SEARCH (Advanced)")
        print("="*60)
        print(f"Query: '{query_text}'")
        print(f"Initial filters: {initial_filters}")
        print(f"Active filters (after ALL removal): {active_filters}")
        print(f"Target results: {max_results}")
        print(f"Search params: nprobes={nprobes}, refine={refine_factor}, overfetch={overfetch_mult}x")
        print("-"*60)
    
    # 1) Try with all active filters
    df, count = execute_search(
        table, model, query_text, active_filters, 
        limit=max_results, overfetch_mult=overfetch_mult,
        refine_factor=refine_factor, nprobes=nprobes
    )
    iterations += 1
    search_history.append({
        'iteration': iterations,
        'filters': active_filters.copy(),
        'count': count
    })
    
    if verbose:
        filter_str = build_filter_string(active_filters) or "No filters"
        print(f"\nðŸ“Š Iteration {iterations}: All filters applied")
        print(f"   SQL: {filter_str}")
        print(f"   Results found: {count}")
    
    # If we have enough results or no filters to relax, return
    if count >= max_results or (count > 0 and len(active_filters) == 0):
        if verbose:
            print(f"\nâœ… Search complete with {count} results")
        
        return {
            'results': df,
            'applied_filters': active_filters,
            'removed_filters': removed_filters,
            'search_iterations': iterations,
            'search_history': search_history,
            'execution_time': time.time() - start_time
        }
    
    # 2) Progressive relaxation in priority order
    for key, value in order_filters_by_priority(active_filters, filter_priority):
        # Remove the filter
        removed_value = active_filters.pop(key, None)
        removed_filters.append({'key': key, 'value': removed_value})
        iterations += 1
        
        # Execute search with relaxed filters
        df, count = execute_search(
            table, model, query_text, active_filters,
            limit=max_results, overfetch_mult=overfetch_mult,
            refine_factor=refine_factor, nprobes=nprobes
        )
        
        search_history.append({
            'iteration': iterations,
            'filters': active_filters.copy(),
            'removed': key,
            'count': count
        })
        
        if verbose:
            filter_str = build_filter_string(active_filters) or "No filters"
            print(f"\nðŸ“Š Iteration {iterations}: Removed '{key}' filter")
            if isinstance(removed_value, list):
                print(f"   Removed values: {removed_value}")
            else:
                print(f"   Removed value: {removed_value}")
            print(f"   SQL: {filter_str}")
            print(f"   Results found: {count}")
        
        # Check if we have enough results
        if count >= max_results or not active_filters:
            if verbose:
                if count >= max_results:
                    print(f"\nâœ… Target reached! Found {count} results")
                else:
                    print(f"\nâš ï¸ All filters removed. Found {count} results")
            break
    
    execution_time = time.time() - start_time
    
    if verbose:
        print("\n" + "="*60)
        print("ðŸ“ˆ SEARCH SUMMARY")
        print("="*60)
        print(f"âœ“ Final result count: {count}")
        print(f"âœ“ Filters applied: {active_filters if active_filters else 'None (pure similarity)'}")
        print(f"âœ“ Filters removed: {[r['key'] for r in removed_filters]}")
        print(f"âœ“ Iterations: {iterations}")
        print(f"âœ“ Execution time: {execution_time:.3f}s")
    
    return {
        'results': df,
        'applied_filters': active_filters,
        'removed_filters': removed_filters,
        'search_iterations': iterations,
        'search_history': search_history,
        'execution_time': execution_time
    }

# ========================================
# SETUP AND TESTING
# ========================================

def setup_test_database():
    """Setup test database with sample data"""
    
    # Sample documents
    documents = [
        {
            'text': 'Singapore ACH payment system for corporate clients in cash management',
            'region': 'ASIA', 'country': 'SINGAPORE', 'topic': 'ACH',
            'product': 'CASH', 'client_type': 'CORPORATES'
        },
        {
            'text': 'Malaysian S2B implementation for retail banking services',
            'region': 'ASIA', 'country': 'MALAYSIA', 'topic': 'S2B',
            'product': 'CASH', 'client_type': 'RETAIL'
        },
        {
            'text': 'SWIFT network for international transfers across Asian markets',
            'region': 'ASIA', 'country': 'ALL', 'topic': 'SWIFT',
            'product': 'FX', 'client_type': 'ALL'
        },
        {
            'text': 'Thai payment infrastructure for corporate FX transactions',
            'region': 'ASIA', 'country': 'THAILAND', 'topic': 'INFRASTRUCTURE',
            'product': 'FX', 'client_type': 'CORPORATES'
        },
        {
            'text': 'Singapore RTGS system for securities settlement',
            'region': 'ASIA', 'country': 'SINGAPORE', 'topic': 'RTGS',
            'product': 'SECURITIES', 'client_type': 'INSTITUTIONAL'
        },
        {
            'text': 'European payment standards implementation in Germany',
            'region': 'EUROPE', 'country': 'GERMANY', 'topic': 'STANDARDS',
            'product': 'CASH', 'client_type': 'CORPORATES'
        },
    ]
    
    # Setup LanceDB
    db = lancedb.connect("./lancedb_advanced")
    
    # Prepare data
    data = []
    for i, doc in enumerate(documents):
        vec = encode_query(model, doc['text'], normalize=True)
        record = {
            'id': i,
            'text': doc['text'],
            'vector': vec,
            **{k: v for k, v in doc.items() if k != 'text'}
        }
        data.append(record)
    
    # Create table
    table_name = "documents"
    if table_name in db.table_names():
        db.drop_table(table_name)
    
    df = pd.DataFrame(data)
    table = db.create_table(table_name, df)
    
    print(f"âœ… Created table with {len(data)} documents")
    return table

# ========================================
# EXAMPLE USAGE
# ========================================

if __name__ == "__main__":
    # Setup database
    table = setup_test_database()
    
    # Example 1: Multi-value filter
    print("\n" + "ðŸš€"*30)
    print("EXAMPLE 1: Multi-Value Filter")
    print("ðŸš€"*30)
    
    filters1 = {
        'country': ['SINGAPORE', 'MALAYSIA', 'THAILAND'],  # Multiple countries
        'product': 'CASH',
        'topic': 'UNKNOWN',  # Will likely need to be relaxed
        'client_type': 'CORPORATES'
    }
    
    result1 = progressive_search(
        table, model,
        query_text="payment systems for corporate clients",
        initial_filters=filters1,
        max_results=5,
        verbose=True
    )
    
    # Example 2: Filter with 'ALL' values
    print("\n" + "ðŸš€"*30)
    print("EXAMPLE 2: Filter with 'ALL' Values")
    print("ðŸš€"*30)
    
    filters2 = {
        'country': 'ALL',  # This will be ignored
        'region': 'ASIA',
        'product': ['CASH', 'FX'],  # Multiple products
        'client_type': 'CORPORATES'
    }
    
    result2 = progressive_search(
        table, model,
        query_text="Asian payment infrastructure",
        initial_filters=filters2,
        max_results=5,
        verbose=True
    )
    
    # Example 3: Very specific filter that needs relaxation
    print("\n" + "ðŸš€"*30)
    print("EXAMPLE 3: Specific Filter Requiring Relaxation")
    print("ðŸš€"*30)
    
    filters3 = {
        'country': 'SINGAPORE',
        'product': 'CRYPTO',  # Doesn't exist
        'topic': 'BLOCKCHAIN',  # Doesn't exist
        'client_type': 'RETAIL',  # Mismatch
        'region': 'ASIA'
    }
    
    result3 = progressive_search(
        table, model,
        query_text="Singapore payment systems",
        initial_filters=filters3,
        max_results=3,
        verbose=True
    )
    
    print("\nâœ… Advanced progressive filter system ready!")

#################################################################
##################################################################
"""
LanceDB Semantic-First Progressive Filter System
Prioritizes query similarity over metadata filters
"""

import lancedb
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import Any, Dict, Optional, List, Tuple
import time
import re

# Initialize model
model = SentenceTransformer('all-MiniLM-L6-v2')

# ========================================
# SEMANTIC ANALYSIS
# ========================================

class QueryIntentAnalyzer:
    """Analyzes query intent for negative conditions and semantic meaning"""
    
    def __init__(self):
        self.negative_patterns = [
            r'without\s+(\w+)',
            r'exclude\s+(\w+)',
            r'excluding\s+(\w+)',
            r'except\s+(\w+)',
            r'not\s+(?:in\s+)?(\w+)',
            r'no\s+(\w+)',
            r'avoid\s+(\w+)',
            r'skip\s+(\w+)',
        ]
        
        # Common countries/regions for entity recognition
        self.countries = {
            'singapore', 'malaysia', 'thailand', 'indonesia', 'philippines',
            'vietnam', 'bangladesh', 'india', 'china', 'japan', 'korea',
            'usa', 'uk', 'germany', 'france', 'australia'
        }
        
        self.regions = {'asia', 'europe', 'americas', 'africa', 'apac', 'emea'}
        self.products = {'cash', 'fx', 'securities', 'trade', 'treasury'}
        self.topics = {'s2b', 'ach', 'rtgs', 'swift', 'wire', 'sepa'}
    
    def extract_negations(self, query: str) -> Dict[str, List[str]]:
        """Extract negative conditions from query"""
        query_lower = query.lower()
        negations = {
            'countries': [],
            'regions': [],
            'products': [],
            'topics': [],
            'general': []
        }
        
        for pattern in self.negative_patterns:
            matches = re.findall(pattern, query_lower)
            for match in matches:
                # Categorize the negation
                if match in self.countries:
                    negations['countries'].append(match.upper())
                elif match in self.regions:
                    negations['regions'].append(match.upper())
                elif match in self.products:
                    negations['products'].append(match.upper())
                elif match in self.topics:
                    negations['topics'].append(match.upper())
                else:
                    negations['general'].append(match)
        
        return negations
    
    def should_override_filter(self, query: str, filter_key: str, filter_value: Any) -> bool:
        """
        Determine if a filter should be overridden based on query semantics
        
        Returns True if the filter contradicts the query intent
        """
        negations = self.extract_negations(query)
        
        # Check if filter value is in negations
        if filter_key == 'country' and filter_value:
            if isinstance(filter_value, str):
                return filter_value.upper() in [n.upper() for n in negations['countries']]
            elif isinstance(filter_value, list):
                return any(v.upper() in [n.upper() for n in negations['countries']] 
                          for v in filter_value)
        
        # Similar checks for other filter types
        if filter_key == 'region' and filter_value:
            if isinstance(filter_value, str):
                return filter_value.upper() in [n.upper() for n in negations['regions']]
        
        return False

# ========================================
# SEMANTIC-AWARE SEARCH
# ========================================

def semantic_priority_search(
    table,
    model,
    query_text: str,
    filters: Dict[str, Any],
    limit: int = 20,
    semantic_threshold: float = 0.7,
    use_two_stage: bool = True,
    verbose: bool = False
) -> Tuple[pd.DataFrame, int, Dict]:
    """
    Two-stage search that prioritizes semantic similarity
    
    Stage 1: Get top candidates by pure similarity (no filters)
    Stage 2: Apply filters as soft constraints with scoring adjustment
    """
    
    analyzer = QueryIntentAnalyzer()
    negations = analyzer.extract_negations(query_text)
    
    # Encode query
    qvec = model.encode(query_text)
    qvec = qvec / np.linalg.norm(qvec)
    
    if use_two_stage:
        # Stage 1: Get more candidates than needed without filters
        candidate_limit = limit * 5  # Get 5x candidates
        
        if verbose:
            print(f"\nðŸ“Š Stage 1: Semantic search for {candidate_limit} candidates")
            print(f"   Negations detected: {negations}")
        
        candidates_df = (
            table.search(qvec)
            .metric("cosine")
            .limit(candidate_limit)
            .to_pandas()
        )
        
        if len(candidates_df) == 0:
            return pd.DataFrame(), 0, {'negations': negations}
        
        # Calculate semantic scores (convert distance to similarity)
        candidates_df['semantic_score'] = 1 - candidates_df['_distance']
        
        # Stage 2: Score adjustment based on filters and negations
        if verbose:
            print(f"\nðŸ“Š Stage 2: Applying smart filtering")
        
        # Apply positive filters as soft constraints
        candidates_df['filter_score'] = 1.0
        
        for key, value in filters.items():
            if value and not analyzer.should_override_filter(query_text, key, value):
                # Apply filter as scoring factor
                if isinstance(value, list):
                    matches = candidates_df[key].isin(value)
                else:
                    matches = candidates_df[key] == value
                
                # Boost score for matches, penalize non-matches
                candidates_df.loc[matches, 'filter_score'] *= 1.2
                candidates_df.loc[~matches, 'filter_score'] *= 0.8
        
        # Apply negation penalties
        for country in negations['countries']:
            mask = candidates_df['country'].str.upper() == country.upper()
            candidates_df.loc[mask, 'filter_score'] *= 0.3  # Heavy penalty
            
        for region in negations['regions']:
            mask = candidates_df['region'].str.upper() == region.upper()
            candidates_df.loc[mask, 'filter_score'] *= 0.3
        
        # Combine scores (semantic takes priority)
        candidates_df['final_score'] = (
            candidates_df['semantic_score'] * semantic_threshold +
            candidates_df['filter_score'] * (1 - semantic_threshold)
        )
        
        # Sort by final score and limit
        results_df = candidates_df.nlargest(limit, 'final_score')
        
        if verbose:
            print(f"   Results after smart filtering: {len(results_df)}")
            print(f"   Top result score: semantic={results_df.iloc[0]['semantic_score']:.3f}, "
                  f"filter={results_df.iloc[0]['filter_score']:.3f}, "
                  f"final={results_df.iloc[0]['final_score']:.3f}")
        
        return results_df, len(results_df), {'negations': negations}
    
    else:
        # Fallback: Traditional filter-based search
        from lancedb_advanced_progressive_filter import build_filter_string, drop_all_values
        
        clean_filters = drop_all_values(filters)
        filter_string = build_filter_string(clean_filters)
        
        search_query = table.search(qvec).metric("cosine")
        if filter_string:
            search_query = search_query.where(filter_string)
        
        results_df = search_query.limit(limit).to_pandas()
        return results_df, len(results_df), {'negations': negations}

# ========================================
# PROGRESSIVE SEARCH WITH SEMANTIC PRIORITY
# ========================================

def progressive_semantic_search(
    table,
    model,
    query_text: str,
    initial_filters: Dict[str, Any],
    max_results: int = 20,
    filter_priority: Dict[str, int] = None,
    semantic_threshold: float = 0.7,
    min_semantic_score: float = 0.3,
    verbose: bool = True
) -> Dict:
    """
    Progressive filter relaxation with semantic priority
    
    Key improvements:
    1. Detects negative conditions in query
    2. Overrides filters that contradict query intent
    3. Uses two-stage retrieval to prioritize semantics
    4. Applies filters as soft constraints
    """
    
    start_time = time.time()
    analyzer = QueryIntentAnalyzer()
    
    # Default priority
    if filter_priority is None:
        filter_priority = {
            "topic": 1,
            "product": 2,
            "client_type": 3,
            "region": 4,
            "country": 5
        }
    
    # Analyze query for negations
    negations = analyzer.extract_negations(query_text)
    
    # Clean filters and check for contradictions
    active_filters = {}
    overridden_filters = {}
    
    for key, value in initial_filters.items():
        if value and value != 'ALL':
            if analyzer.should_override_filter(query_text, key, value):
                overridden_filters[key] = value
                if verbose:
                    print(f"âš ï¸ Overriding filter {key}={value} due to query negation")
            else:
                active_filters[key] = value
    
    if verbose:
        print("\n" + "="*60)
        print("ðŸ” SEMANTIC-FIRST PROGRESSIVE SEARCH")
        print("="*60)
        print(f"Query: '{query_text}'")
        print(f"Detected negations: {negations}")
        print(f"Initial filters: {initial_filters}")
        print(f"Active filters: {active_filters}")
        print(f"Overridden filters: {overridden_filters}")
        print(f"Semantic threshold: {semantic_threshold}")
        print("-"*60)
    
    iterations = 0
    removed_filters = []
    search_history = []
    
    # Try with current filters
    while True:
        iterations += 1
        
        # Perform semantic-priority search
        results_df, count, metadata = semantic_priority_search(
            table, model, query_text, active_filters,
            limit=max_results, 
            semantic_threshold=semantic_threshold,
            use_two_stage=True,
            verbose=verbose and iterations == 1
        )
        
        search_history.append({
            'iteration': iterations,
            'filters': active_filters.copy(),
            'count': count
        })
        
        if verbose:
            print(f"\nðŸ“Š Iteration {iterations}: ")
            print(f"   Active filters: {list(active_filters.keys()) if active_filters else 'None'}")
            print(f"   Results found: {count}")
            if count > 0 and 'semantic_score' in results_df.columns:
                avg_semantic = results_df['semantic_score'].mean()
                print(f"   Avg semantic score: {avg_semantic:.3f}")
        
        # Check termination conditions
        if count >= max_results:
            if verbose:
                print(f"\nâœ… Target reached with {count} results")
            break
        
        if not active_filters:
            if verbose:
                print(f"\nâš ï¸ All filters removed. Returning {count} results")
            break
        
        # Remove lowest priority filter
        if active_filters:
            filters_by_priority = sorted(
                active_filters.items(),
                key=lambda x: filter_priority.get(x[0], 999)
            )
            
            if filters_by_priority:
                key_to_remove, value_removed = filters_by_priority[0]
                del active_filters[key_to_remove]
                removed_filters.append({'key': key_to_remove, 'value': value_removed})
                
                if verbose:
                    print(f"   â†’ Removing filter: {key_to_remove}={value_removed}")
        else:
            break
    
    # Filter results by minimum semantic score
    if count > 0 and 'semantic_score' in results_df.columns:
        results_df = results_df[results_df['semantic_score'] >= min_semantic_score]
        if verbose and len(results_df) < count:
            print(f"\nðŸ“ Filtered {count - len(results_df)} results below semantic threshold {min_semantic_score}")
    
    execution_time = time.time() - start_time
    
    if verbose:
        print("\n" + "="*60)
        print("ðŸ“ˆ SEARCH SUMMARY")
        print("="*60)
        print(f"âœ“ Final result count: {len(results_df)}")
        print(f"âœ“ Query negations: {negations}")
        print(f"âœ“ Overridden filters: {list(overridden_filters.keys())}")
        print(f"âœ“ Applied filters: {active_filters if active_filters else 'None'}")
        print(f"âœ“ Removed filters: {[r['key'] for r in removed_filters]}")
        print(f"âœ“ Iterations: {iterations}")
        print(f"âœ“ Execution time: {execution_time:.3f}s")
    
    return {
        'results': results_df,
        'applied_filters': active_filters,
        'removed_filters': removed_filters,
        'overridden_filters': overridden_filters,
        'negations': negations,
        'iterations': iterations,
        'execution_time': execution_time
    }

# ========================================
# TESTING
# ========================================

def setup_test_database_with_countries():
    """Setup test database with country-specific data"""
    
    documents = [
        {
            'text': 'S2B payment services available in Singapore for corporate clients',
            'region': 'ASIA', 'country': 'SINGAPORE', 'topic': 'S2B',
            'product': 'CASH', 'client_type': 'CORPORATES'
        },
        {
            'text': 'S2B implementation across all Asian countries including Bangladesh',
            'region': 'ASIA', 'country': 'BANGLADESH', 'topic': 'S2B',
            'product': 'CASH', 'client_type': 'CORPORATES'
        },
        {
            'text': 'S2B payment network for Malaysia Thailand and Indonesia',
            'region': 'ASIA', 'country': 'MALAYSIA', 'topic': 'S2B',
            'product': 'CASH', 'client_type': 'CORPORATES'
        },
        {
            'text': 'ACH system implementation in Bangladesh for retail banking',
            'region': 'ASIA', 'country': 'BANGLADESH', 'topic': 'ACH',
            'product': 'CASH', 'client_type': 'RETAIL'
        },
        {
            'text': 'SWIFT network connecting Asian markets excluding certain countries',
            'region': 'ASIA', 'country': 'ALL', 'topic': 'SWIFT',
            'product': 'FX', 'client_type': 'ALL'
        },
        {
            'text': 'S2B corporate solutions for APAC region without Bangladesh market',
            'region': 'ASIA', 'country': 'ALL', 'topic': 'S2B',
            'product': 'CASH', 'client_type': 'CORPORATES'
        },
    ]
    
    # Setup LanceDB
    db = lancedb.connect("./lancedb_semantic")
    
    # Prepare data
    data = []
    for i, doc in enumerate(documents):
        vec = model.encode(doc['text'])
        vec = vec / np.linalg.norm(vec)
        record = {
            'id': i,
            'text': doc['text'],
            'vector': vec,
            **{k: v for k, v in doc.items() if k != 'text'}
        }
        data.append(record)
    
    # Create table
    table_name = "documents"
    if table_name in db.table_names():
        db.drop_table(table_name)
    
    df = pd.DataFrame(data)
    table = db.create_table(table_name, df)
    
    print(f"âœ… Created table with {len(data)} documents")
    return table

if __name__ == "__main__":
    # Setup database
    table = setup_test_database_with_countries()
    
    # Test 1: Query with negative condition
    print("\n" + "ðŸš€"*30)
    print("TEST 1: Query with Negative Condition")
    print("ðŸš€"*30)
    
    # The query asks for S2B WITHOUT Bangladesh
    # But the filter might extract Bangladesh as country
    query1 = "S2B payment capability without Bangladesh"
    filters1 = {
        'country': 'BANGLADESH',  # This contradicts the query!
        'topic': 'S2B',
        'client_type': 'CORPORATES'
    }
    
    result1 = progressive_semantic_search(
        table, model,
        query_text=query1,
        initial_filters=filters1,
        max_results=5,
        semantic_threshold=0.7,  # 70% weight to semantic similarity
        verbose=True
    )
    
    print("\nðŸ” Results:")
    if not result1['results'].empty:
        for idx, row in result1['results'].head().iterrows():
            print(f"\nðŸ“„ Result {idx+1}:")
            print(f"   Text: {row['text'][:80]}...")
            print(f"   Country: {row['country']}")
            if 'semantic_score' in row:
                print(f"   Semantic score: {row['semantic_score']:.3f}")
            if 'final_score' in row:
                print(f"   Final score: {row['final_score']:.3f}")
    
    # Test 2: Query asking for exclusion
    print("\n" + "ðŸš€"*30)
    print("TEST 2: Query with Multiple Exclusions")
    print("ðŸš€"*30)
    
    query2 = "Asian payment systems excluding Bangladesh and India"
    filters2 = {
        'region': 'ASIA',
        'topic': 'S2B'
    }
    
    result2 = progressive_semantic_search(
        table, model,
        query_text=query2,
        initial_filters=filters2,
        max_results=5,
        semantic_threshold=0.75,
        verbose=True
    )
    
    # Test 3: Positive query matching filter
    print("\n" + "ðŸš€"*30)
    print("TEST 3: Positive Query (Filter Aligned)")
    print("ðŸš€"*30)
    
    query3 = "S2B payment in Singapore"
    filters3 = {
        'country': 'SINGAPORE',
        'topic': 'S2B'
    }
    
    result3 = progressive_semantic_search(
        table, model,
        query_text=query3,
        initial_filters=filters3,
        max_results=3,
        semantic_threshold=0.6,
        verbose=True
    )
    
    print("\nâœ… Semantic-first progressive search complete!")

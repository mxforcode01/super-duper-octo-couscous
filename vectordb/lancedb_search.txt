EMB_MODEL = SentenceTransformer(embedding_path)  # single source of truth

def encode_text(text: str, normalize: bool = False):
    v = EMB_MODEL.encode(text)
    if normalize:
        # cosine is scale-invariant if both sides normalized; keep it consistent
        import numpy as np
        n = np.linalg.norm(v)
        if n > 0:
            v = v / n
    return v

def make_query_text(q: str) -> str:
    return "query: " + str(q)  # matches your Qdrant behavior

def dump_into_lancedb(text_chunks, db_path, table_name):
    db = lancedb.connect(db_path)
    if table_name in db.table_names():
        db.drop_table(table_name)

    rows = []
    for i, ch in enumerate(text_chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            "vector": encode_text(ch["text"], normalize=True),  # normalize on write
            **ch["metadata"]
        })
    df = pd.DataFrame(rows)
    tbl = db.create_table(table_name, df)

    # Build an ANN index with cosine metric.
    # If your dataset is small (<~100k), IVF_PQ is still fine; bump recall with nprobes later.
    # If you prefer exact baseline first, comment this out and rely on flat scan while testing.
    tbl.create_index(
        column="vector",
        index_type="ivf_pq",          # or "ivf_flat" / "hnsw" if available in your version
        metric="cosine",
        num_partitions=64,            # tune for your data size (e.g., ~sqrt(N))
        num_sub_vectors=16            # PQ granularity; raise for better recall
    )

def lancedb_where_clause(filter_dict: dict | None) -> str | None:
    if not filter_dict:
        return None
    parts = []
    for k, v in filter_dict.items():
        if v is None:
            continue
        # allow list-like values
        if isinstance(v, (list, tuple, set)):
            vv = [f"'{str(x).replace(\"'\", \"''\")}'" for x in v]
            parts.append(f"{k} IN ({', '.join(vv)})")
        else:
            s = str(v).replace("'", "''")
            parts.append(f"{k} = '{s}'")
    return " AND ".join(parts) if parts else None

def search_lancedb(query: str, table, top_k: int = 20, filter_dict: dict | None = None):
    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True)

    # Build search
    sq = table.search(qvec).metric("cosine")

    # Prefilter BEFORE the ANN probe (closer to Qdrant‚Äôs integrated filter)
    where = lancedb_where_clause(filter_dict)
    if where:
        sq = sq.where(where)  # LanceDB treats this as prefilter for the vector scan

    # Overfetch similar to your Qdrant loop and refine
    over_k = max(top_k * 3, top_k + 10)   # extra headroom
    sq = sq.limit(over_k).refine_factor(3)   # re-rank a larger candidate set exactly

    # Probe more partitions (higher recall; slower). Tune 8‚Äì32.
    sq = sq.nprobes(16)

    df = sq.to_pandas()

    # Clip to final top_k (already ranked by distance)
    return df.head(top_k)

###########################################################
# Qdrant
from qdrant_client.http.models import SearchParams

def dump_into_qdrant(chunks, collection_name="my_docs_bge"):
    client = QdrantClient(path=qdrant_path)
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=768, distance=Distance.COSINE)
    )

    rows = []
    for i, ch in enumerate(chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            **ch["metadata"]
        })

    texts = [r["text"] for r in rows]
    payloads = rows
    vectors = [encode_text(t, normalize=True).tolist() for t in texts]  # normalize on write (optional but consistent)

    ids = [r["id"] for r in rows]

    client.upload_collection(
        collection_name=collection_name,
        vectors=vectors,
        payload=payloads,
        ids=ids,
    )
    return

def search_qdrant(query, client, collection_name="my_docs_bge", top_k=20, stages: list[dict] | None=None):
    if stages is None:
        stages = [{}]

    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True).tolist()

    results = []
    seen_ids = set()
    for stage in stages:
        need = top_k - len(results)
        if need <= 0:
            break
        flt = make_filter(stage)
        hits = client.search(
            collection_name=collection_name,
            query_vector=qvec,
            query_filter=flt,
            limit=max(need*3, need+10),
            with_payload=True,
            params=SearchParams(hnsw_ef=128, exact=False)  # make ‚Äúdefault‚Äù recall explicit
        )
        for pt in hits:
            if pt.id not in seen_ids:
                results.append(pt)
                seen_ids.add(pt.id)
                if len(results) >= top_k:
                    break
    return results

##############################################################################################

import numpy as np

from typing import Any, Dict, Optional

def _is_all(v: Any) -> bool:
    return isinstance(v, str) and v.strip().upper() == "ALL"

def _sql_quote(v: Any) -> str:
    # Quote strings safely (double single quotes); pass numbers/bools through.
    if isinstance(v, str):
        return "'" + v.replace("'", "''") + "'"
    if isinstance(v, bool):
        return "TRUE" if v else "FALSE"
    return str(v)

def build_filter_string(filters: Dict[str, Any]) -> Optional[str]:
    """
    Build a LanceDB SQL-like WHERE clause from dict values.
    Supports scalars and sequences. If any value in a key's list is 'ALL',
    that key is skipped (no filtering on that field).
    Examples:
      {'country': ['SINGAPORE','MALAYSIA'], 'product':['CASH']}
         -> "country IN ('SINGAPORE','MALAYSIA') AND product = 'CASH'"
    """
    if not filters:
        return None

    clauses = []
    for key, raw in filters.items():
        if raw is None:
            continue

        # Normalize to a list for uniform handling
        if isinstance(raw, (list, tuple, set)):
            values = [v for v in raw if v is not None and (not isinstance(v, str) or v.strip() != "")]
        else:
            values = [raw]

        if not values:
            continue

        # If ANY value is 'ALL', skip this key entirely (no filter on it)
        if any(_is_all(v) for v in values):
            continue

        # De-dupe while preserving order (case-insensitive for strings)
        seen = set()
        deduped = []
        for v in values:
            key_v = v.upper() if isinstance(v, str) else v
            if key_v in seen:
                continue
            seen.add(key_v)
            deduped.append(v)

        # Emit = or IN (...)
        if len(deduped) == 1:
            clauses.append(f"{key} = {_sql_quote(deduped[0])}")
        else:
            joined = ", ".join(_sql_quote(v) for v in deduped)
            clauses.append(f"{key} IN ({joined})")

    return " AND ".join(clauses) if clauses else None


def execute_search(
    table,
    model,
    query_text: str,
    filters: Dict[str, object],
    limit: int = 20,
    overfetch_mult: int = 3,
    refine_factor: int = 3,
    nprobes: int = 16,
) -> Tuple[pd.DataFrame, int]:
    """
    Qdrant-like LanceDB search:
    - cosine metric
    - normalized query vector
    - prefilter via .where(...)
    - overfetch + refine + nprobes to raise recall
    """
    qtxt = make_query_text(query_text)
    qvec = encode_query(model, qtxt, normalize=True)

    # Drop any 'ALL' values to match your usual semantics
    filters = drop_all_values(filters)
    fstr = build_filter_string(filters)

    # Overfetch similar to your Qdrant loop (need * 3)
    over_k = max(limit * overfetch_mult, limit + 10)

    sq = table.search(qvec).metric("cosine")
    if fstr:
        sq = sq.where(fstr)           # PREFILTER: affect the candidate set
    sq = sq.nprobes(nprobes)          # probe more partitions (higher recall)
    sq = sq.refine_factor(refine_factor)  # re-rank a larger candidate set exactly
    sq = sq.limit(over_k)             # over-fetch then clip

    df = sq.to_pandas()
    # NOTE: LanceDB returns already sorted results (by distance/score).
    # Clip to requested limit:
    df = df.head(limit)
    return df, len(df)

FILTER_PRIORITY_DEFAULT = {
    "topic": 1,
    "product": 2,
    "client_type": 3,
    "region": 4,
    "country": 5,
}

def clean_filters(filters: Dict[str, str]) -> Dict[str, str]:
    return {k: v for k, v in (filters or {}).items() if v is not None and str(v) != ""}

def order_filters_by_priority(
    active_filters: Dict[str, str],
    priority_map: Dict[str, int] = FILTER_PRIORITY_DEFAULT
) -> List[Tuple[str, str]]:
    return sorted(active_filters.items(), key=lambda kv: priority_map.get(kv[0], 999))

def progressive_search(
    table,
    model,
    query_text: str,
    initial_filters: Dict[str, str],
    max_results: int = 20,
    filter_priority: Dict[str, int] = FILTER_PRIORITY_DEFAULT,
    verbose: bool = True
) -> pd.DataFrame:
    active_filters = drop_all_values(clean_filters(initial_filters.copy()))
    iterations = 0

    # 1) Try with all filters
    df, count = execute_search(table, model, query_text, active_filters, limit=max_results)
    iterations += 1
    if count >= max_results or (count > 0 and len(active_filters) == 0):
        return df

    # 2) Relax in priority order (topic ‚Üí product ‚Üí client_type ‚Üí region ‚Üí country)
    for key, _ in order_filters_by_priority(active_filters, filter_priority):
        active_filters.pop(key, None)
        iterations += 1
        df, count = execute_search(table, model, query_text, active_filters, limit=max_results)
        if count >= max_results or not active_filters:
            break

    return df

################################################################################
# Claude 
"""
LanceDB Advanced Progressive Filter System
Integrates multi-value filtering, "ALL" handling, and optimized search
"""

import lancedb
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import Any, Dict, Optional, List, Tuple
import time

# Initialize model
model = SentenceTransformer('all-MiniLM-L6-v2')

# ========================================
# CORE FILTERING UTILITIES
# ========================================

def _is_all(v: Any) -> bool:
    """Check if a value represents 'ALL' (no filtering)"""
    return isinstance(v, str) and v.strip().upper() == "ALL"

def _sql_quote(v: Any) -> str:
    """Quote values safely for SQL-like queries"""
    if isinstance(v, str):
        return "'" + v.replace("'", "''") + "'"
    if isinstance(v, bool):
        return "TRUE" if v else "FALSE"
    return str(v)

def build_filter_string(filters: Dict[str, Any]) -> Optional[str]:
    """
    Build a LanceDB SQL-like WHERE clause from dict values.
    Supports scalars and sequences. If any value in a key's list is 'ALL',
    that key is skipped (no filtering on that field).
    
    Examples:
      {'country': ['SINGAPORE','MALAYSIA'], 'product': 'CASH'}
         -> "country IN ('SINGAPORE','MALAYSIA') AND product = 'CASH'"
      {'country': 'ALL', 'product': 'CASH'}
         -> "product = 'CASH'" (country is skipped)
    """
    if not filters:
        return None

    clauses = []
    for key, raw in filters.items():
        if raw is None:
            continue

        # Normalize to a list for uniform handling
        if isinstance(raw, (list, tuple, set)):
            values = [v for v in raw if v is not None and (not isinstance(v, str) or v.strip() != "")]
        else:
            values = [raw] if raw != "" else []

        if not values:
            continue

        # If ANY value is 'ALL', skip this key entirely (no filter on it)
        if any(_is_all(v) for v in values):
            continue

        # De-duplicate while preserving order (case-insensitive for strings)
        seen = set()
        deduped = []
        for v in values:
            key_v = v.upper() if isinstance(v, str) else v
            if key_v in seen:
                continue
            seen.add(key_v)
            deduped.append(v)

        # Emit = or IN (...)
        if len(deduped) == 1:
            clauses.append(f"{key} = {_sql_quote(deduped[0])}")
        else:
            joined = ", ".join(_sql_quote(v) for v in deduped)
            clauses.append(f"{key} IN ({joined})")

    return " AND ".join(clauses) if clauses else None

def drop_all_values(filters: Dict[str, Any]) -> Dict[str, Any]:
    """Remove any filter keys that have 'ALL' as their value"""
    if not filters:
        return {}
    
    cleaned = {}
    for key, value in filters.items():
        # Check if it's 'ALL' (single value or in a list)
        if isinstance(value, (list, tuple, set)):
            non_all_values = [v for v in value if not _is_all(v)]
            if non_all_values:  # Only keep if there are non-ALL values
                cleaned[key] = non_all_values if len(non_all_values) > 1 else non_all_values[0]
        elif not _is_all(value):
            cleaned[key] = value
    
    return cleaned

def clean_filters(filters: Dict[str, Any]) -> Dict[str, Any]:
    """Clean filters by removing None and empty string values"""
    return {k: v for k, v in (filters or {}).items() 
            if v is not None and (not isinstance(v, str) or v.strip() != "")}

def encode_query(model, query_text: str, normalize: bool = True) -> np.ndarray:
    """Encode query text to vector, optionally normalize for cosine similarity"""
    vec = model.encode(query_text)
    if normalize:
        vec = vec / np.linalg.norm(vec)
    return vec

# ========================================
# SEARCH EXECUTION
# ========================================

def execute_search(
    table,
    model,
    query_text: str,
    filters: Dict[str, Any],
    limit: int = 20,
    overfetch_mult: int = 3,
    refine_factor: int = 3,
    nprobes: int = 20,
) -> Tuple[pd.DataFrame, int]:
    """
    Optimized LanceDB search with:
    - Cosine similarity metric
    - Normalized query vector
    - Pre-filtering via where clause
    - Overfetch + refine + nprobes for higher recall
    
    Args:
        table: LanceDB table
        model: Sentence transformer model
        query_text: Search query
        filters: Filter dictionary (supports lists and 'ALL' values)
        limit: Maximum results to return
        overfetch_mult: Multiplier for initial candidate retrieval
        refine_factor: Re-ranking factor for accuracy
        nprobes: Number of partitions to search (higher = better recall)
    """
    # Encode and normalize query
    qvec = encode_query(model, query_text, normalize=True)
    
    # Clean filters and build SQL string
    filters = drop_all_values(filters)
    filter_string = build_filter_string(filters)
    
    # Overfetch to ensure we get enough results after filtering
    over_k = max(limit * overfetch_mult, limit + 10)
    
    # Build search query
    search_query = table.search(qvec).metric("cosine")
    
    if filter_string:
        search_query = search_query.where(filter_string)
    
    search_query = (search_query
                   .nprobes(nprobes)           # Probe more partitions
                   .refine_factor(refine_factor)  # Re-rank candidates
                   .limit(over_k))                # Over-fetch
    
    # Execute search
    df = search_query.to_pandas()
    
    # Clip to requested limit
    df = df.head(limit)
    return df, len(df)

# ========================================
# PROGRESSIVE FILTER SYSTEM
# ========================================

# Default filter priority (lower number = removed first)
FILTER_PRIORITY_DEFAULT = {
    "topic": 1,        # Removed first
    "product": 2,
    "client_type": 3,
    "region": 4,
    "country": 5,      # Removed last (most important)
}

def order_filters_by_priority(
    active_filters: Dict[str, Any],
    priority_map: Dict[str, int] = None
) -> List[Tuple[str, Any]]:
    """Order filters by priority for progressive relaxation"""
    if priority_map is None:
        priority_map = FILTER_PRIORITY_DEFAULT
    return sorted(active_filters.items(), 
                 key=lambda kv: priority_map.get(kv[0], 999))

def progressive_search(
    table,
    model,
    query_text: str,
    initial_filters: Dict[str, Any],
    max_results: int = 20,
    filter_priority: Dict[str, int] = None,
    verbose: bool = True,
    overfetch_mult: int = 3,
    refine_factor: int = 3,
    nprobes: int = 20,
) -> Dict:
    """
    Progressive filter relaxation search with advanced filtering support.
    
    Supports:
    - Multi-value filters: {'country': ['SINGAPORE', 'MALAYSIA']}
    - 'ALL' values that disable filtering: {'country': 'ALL'}
    - Progressive relaxation based on priority
    - Optimized search with overfetch and refinement
    
    Args:
        table: LanceDB table
        model: Sentence transformer model
        query_text: Search query
        initial_filters: Initial filter dictionary
        max_results: Target number of results
        filter_priority: Custom priority map (optional)
        verbose: Print detailed progress
        overfetch_mult: Overfetch multiplier for better recall
        refine_factor: Refinement factor for re-ranking
        nprobes: Number of partitions to probe
        
    Returns:
        Dictionary with results and metadata
    """
    start_time = time.time()
    
    if filter_priority is None:
        filter_priority = FILTER_PRIORITY_DEFAULT
    
    # Clean and prepare filters
    active_filters = drop_all_values(clean_filters(initial_filters.copy()))
    removed_filters = []
    iterations = 0
    search_history = []
    
    if verbose:
        print("\n" + "="*60)
        print("üîç PROGRESSIVE FILTER SEARCH (Advanced)")
        print("="*60)
        print(f"Query: '{query_text}'")
        print(f"Initial filters: {initial_filters}")
        print(f"Active filters (after ALL removal): {active_filters}")
        print(f"Target results: {max_results}")
        print(f"Search params: nprobes={nprobes}, refine={refine_factor}, overfetch={overfetch_mult}x")
        print("-"*60)
    
    # 1) Try with all active filters
    df, count = execute_search(
        table, model, query_text, active_filters, 
        limit=max_results, overfetch_mult=overfetch_mult,
        refine_factor=refine_factor, nprobes=nprobes
    )
    iterations += 1
    search_history.append({
        'iteration': iterations,
        'filters': active_filters.copy(),
        'count': count
    })
    
    if verbose:
        filter_str = build_filter_string(active_filters) or "No filters"
        print(f"\nüìä Iteration {iterations}: All filters applied")
        print(f"   SQL: {filter_str}")
        print(f"   Results found: {count}")
    
    # If we have enough results or no filters to relax, return
    if count >= max_results or (count > 0 and len(active_filters) == 0):
        if verbose:
            print(f"\n‚úÖ Search complete with {count} results")
        
        return {
            'results': df,
            'applied_filters': active_filters,
            'removed_filters': removed_filters,
            'search_iterations': iterations,
            'search_history': search_history,
            'execution_time': time.time() - start_time
        }
    
    # 2) Progressive relaxation in priority order
    for key, value in order_filters_by_priority(active_filters, filter_priority):
        # Remove the filter
        removed_value = active_filters.pop(key, None)
        removed_filters.append({'key': key, 'value': removed_value})
        iterations += 1
        
        # Execute search with relaxed filters
        df, count = execute_search(
            table, model, query_text, active_filters,
            limit=max_results, overfetch_mult=overfetch_mult,
            refine_factor=refine_factor, nprobes=nprobes
        )
        
        search_history.append({
            'iteration': iterations,
            'filters': active_filters.copy(),
            'removed': key,
            'count': count
        })
        
        if verbose:
            filter_str = build_filter_string(active_filters) or "No filters"
            print(f"\nüìä Iteration {iterations}: Removed '{key}' filter")
            if isinstance(removed_value, list):
                print(f"   Removed values: {removed_value}")
            else:
                print(f"   Removed value: {removed_value}")
            print(f"   SQL: {filter_str}")
            print(f"   Results found: {count}")
        
        # Check if we have enough results
        if count >= max_results or not active_filters:
            if verbose:
                if count >= max_results:
                    print(f"\n‚úÖ Target reached! Found {count} results")
                else:
                    print(f"\n‚ö†Ô∏è All filters removed. Found {count} results")
            break
    
    execution_time = time.time() - start_time
    
    if verbose:
        print("\n" + "="*60)
        print("üìà SEARCH SUMMARY")
        print("="*60)
        print(f"‚úì Final result count: {count}")
        print(f"‚úì Filters applied: {active_filters if active_filters else 'None (pure similarity)'}")
        print(f"‚úì Filters removed: {[r['key'] for r in removed_filters]}")
        print(f"‚úì Iterations: {iterations}")
        print(f"‚úì Execution time: {execution_time:.3f}s")
    
    return {
        'results': df,
        'applied_filters': active_filters,
        'removed_filters': removed_filters,
        'search_iterations': iterations,
        'search_history': search_history,
        'execution_time': execution_time
    }

# ========================================
# SETUP AND TESTING
# ========================================

def setup_test_database():
    """Setup test database with sample data"""
    
    # Sample documents
    documents = [
        {
            'text': 'Singapore ACH payment system for corporate clients in cash management',
            'region': 'ASIA', 'country': 'SINGAPORE', 'topic': 'ACH',
            'product': 'CASH', 'client_type': 'CORPORATES'
        },
        {
            'text': 'Malaysian S2B implementation for retail banking services',
            'region': 'ASIA', 'country': 'MALAYSIA', 'topic': 'S2B',
            'product': 'CASH', 'client_type': 'RETAIL'
        },
        {
            'text': 'SWIFT network for international transfers across Asian markets',
            'region': 'ASIA', 'country': 'ALL', 'topic': 'SWIFT',
            'product': 'FX', 'client_type': 'ALL'
        },
        {
            'text': 'Thai payment infrastructure for corporate FX transactions',
            'region': 'ASIA', 'country': 'THAILAND', 'topic': 'INFRASTRUCTURE',
            'product': 'FX', 'client_type': 'CORPORATES'
        },
        {
            'text': 'Singapore RTGS system for securities settlement',
            'region': 'ASIA', 'country': 'SINGAPORE', 'topic': 'RTGS',
            'product': 'SECURITIES', 'client_type': 'INSTITUTIONAL'
        },
        {
            'text': 'European payment standards implementation in Germany',
            'region': 'EUROPE', 'country': 'GERMANY', 'topic': 'STANDARDS',
            'product': 'CASH', 'client_type': 'CORPORATES'
        },
    ]
    
    # Setup LanceDB
    db = lancedb.connect("./lancedb_advanced")
    
    # Prepare data
    data = []
    for i, doc in enumerate(documents):
        vec = encode_query(model, doc['text'], normalize=True)
        record = {
            'id': i,
            'text': doc['text'],
            'vector': vec,
            **{k: v for k, v in doc.items() if k != 'text'}
        }
        data.append(record)
    
    # Create table
    table_name = "documents"
    if table_name in db.table_names():
        db.drop_table(table_name)
    
    df = pd.DataFrame(data)
    table = db.create_table(table_name, df)
    
    print(f"‚úÖ Created table with {len(data)} documents")
    return table

# ========================================
# EXAMPLE USAGE
# ========================================

if __name__ == "__main__":
    # Setup database
    table = setup_test_database()
    
    # Example 1: Multi-value filter
    print("\n" + "üöÄ"*30)
    print("EXAMPLE 1: Multi-Value Filter")
    print("üöÄ"*30)
    
    filters1 = {
        'country': ['SINGAPORE', 'MALAYSIA', 'THAILAND'],  # Multiple countries
        'product': 'CASH',
        'topic': 'UNKNOWN',  # Will likely need to be relaxed
        'client_type': 'CORPORATES'
    }
    
    result1 = progressive_search(
        table, model,
        query_text="payment systems for corporate clients",
        initial_filters=filters1,
        max_results=5,
        verbose=True
    )
    
    # Example 2: Filter with 'ALL' values
    print("\n" + "üöÄ"*30)
    print("EXAMPLE 2: Filter with 'ALL' Values")
    print("üöÄ"*30)
    
    filters2 = {
        'country': 'ALL',  # This will be ignored
        'region': 'ASIA',
        'product': ['CASH', 'FX'],  # Multiple products
        'client_type': 'CORPORATES'
    }
    
    result2 = progressive_search(
        table, model,
        query_text="Asian payment infrastructure",
        initial_filters=filters2,
        max_results=5,
        verbose=True
    )
    
    # Example 3: Very specific filter that needs relaxation
    print("\n" + "üöÄ"*30)
    print("EXAMPLE 3: Specific Filter Requiring Relaxation")
    print("üöÄ"*30)
    
    filters3 = {
        'country': 'SINGAPORE',
        'product': 'CRYPTO',  # Doesn't exist
        'topic': 'BLOCKCHAIN',  # Doesn't exist
        'client_type': 'RETAIL',  # Mismatch
        'region': 'ASIA'
    }
    
    result3 = progressive_search(
        table, model,
        query_text="Singapore payment systems",
        initial_filters=filters3,
        max_results=3,
        verbose=True
    )
    
    print("\n‚úÖ Advanced progressive filter system ready!")

EMB_MODEL = SentenceTransformer(embedding_path)  # single source of truth

def encode_text(text: str, normalize: bool = False):
    v = EMB_MODEL.encode(text)
    if normalize:
        # cosine is scale-invariant if both sides normalized; keep it consistent
        import numpy as np
        n = np.linalg.norm(v)
        if n > 0:
            v = v / n
    return v

def make_query_text(q: str) -> str:
    return "query: " + str(q)  # matches your Qdrant behavior

def dump_into_lancedb(text_chunks, db_path, table_name):
    db = lancedb.connect(db_path)
    if table_name in db.table_names():
        db.drop_table(table_name)

    rows = []
    for i, ch in enumerate(text_chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            "vector": encode_text(ch["text"], normalize=True),  # normalize on write
            **ch["metadata"]
        })
    df = pd.DataFrame(rows)
    tbl = db.create_table(table_name, df)

    # Build an ANN index with cosine metric.
    # If your dataset is small (<~100k), IVF_PQ is still fine; bump recall with nprobes later.
    # If you prefer exact baseline first, comment this out and rely on flat scan while testing.
    tbl.create_index(
        column="vector",
        index_type="ivf_pq",          # or "ivf_flat" / "hnsw" if available in your version
        metric="cosine",
        num_partitions=64,            # tune for your data size (e.g., ~sqrt(N))
        num_sub_vectors=16            # PQ granularity; raise for better recall
    )

def lancedb_where_clause(filter_dict: dict | None) -> str | None:
    if not filter_dict:
        return None
    parts = []
    for k, v in filter_dict.items():
        if v is None:
            continue
        # allow list-like values
        if isinstance(v, (list, tuple, set)):
            vv = [f"'{str(x).replace(\"'\", \"''\")}'" for x in v]
            parts.append(f"{k} IN ({', '.join(vv)})")
        else:
            s = str(v).replace("'", "''")
            parts.append(f"{k} = '{s}'")
    return " AND ".join(parts) if parts else None

def search_lancedb(query: str, table, top_k: int = 20, filter_dict: dict | None = None):
    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True)

    # Build search
    sq = table.search(qvec).metric("cosine")

    # Prefilter BEFORE the ANN probe (closer to Qdrant’s integrated filter)
    where = lancedb_where_clause(filter_dict)
    if where:
        sq = sq.where(where)  # LanceDB treats this as prefilter for the vector scan

    # Overfetch similar to your Qdrant loop and refine
    over_k = max(top_k * 3, top_k + 10)   # extra headroom
    sq = sq.limit(over_k).refine_factor(3)   # re-rank a larger candidate set exactly

    # Probe more partitions (higher recall; slower). Tune 8–32.
    sq = sq.nprobes(16)

    df = sq.to_pandas()

    # Clip to final top_k (already ranked by distance)
    return df.head(top_k)

###########################################################
# Qdrant
from qdrant_client.http.models import SearchParams

def dump_into_qdrant(chunks, collection_name="my_docs_bge"):
    client = QdrantClient(path=qdrant_path)
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=768, distance=Distance.COSINE)
    )

    rows = []
    for i, ch in enumerate(chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            **ch["metadata"]
        })

    texts = [r["text"] for r in rows]
    payloads = rows
    vectors = [encode_text(t, normalize=True).tolist() for t in texts]  # normalize on write (optional but consistent)

    ids = [r["id"] for r in rows]

    client.upload_collection(
        collection_name=collection_name,
        vectors=vectors,
        payload=payloads,
        ids=ids,
    )
    return

def search_qdrant(query, client, collection_name="my_docs_bge", top_k=20, stages: list[dict] | None=None):
    if stages is None:
        stages = [{}]

    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True).tolist()

    results = []
    seen_ids = set()
    for stage in stages:
        need = top_k - len(results)
        if need <= 0:
            break
        flt = make_filter(stage)
        hits = client.search(
            collection_name=collection_name,
            query_vector=qvec,
            query_filter=flt,
            limit=max(need*3, need+10),
            with_payload=True,
            params=SearchParams(hnsw_ef=128, exact=False)  # make “default” recall explicit
        )
        for pt in hits:
            if pt.id not in seen_ids:
                results.append(pt)
                seen_ids.add(pt.id)
                if len(results) >= top_k:
                    break
    return results

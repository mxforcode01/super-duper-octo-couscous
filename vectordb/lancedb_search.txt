EMB_MODEL = SentenceTransformer(embedding_path)  # single source of truth

def encode_text(text: str, normalize: bool = False):
    v = EMB_MODEL.encode(text)
    if normalize:
        # cosine is scale-invariant if both sides normalized; keep it consistent
        import numpy as np
        n = np.linalg.norm(v)
        if n > 0:
            v = v / n
    return v

def make_query_text(q: str) -> str:
    return "query: " + str(q)  # matches your Qdrant behavior

def dump_into_lancedb(text_chunks, db_path, table_name):
    db = lancedb.connect(db_path)
    if table_name in db.table_names():
        db.drop_table(table_name)

    rows = []
    for i, ch in enumerate(text_chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            "vector": encode_text(ch["text"], normalize=True),  # normalize on write
            **ch["metadata"]
        })
    df = pd.DataFrame(rows)
    tbl = db.create_table(table_name, df)

    # Build an ANN index with cosine metric.
    # If your dataset is small (<~100k), IVF_PQ is still fine; bump recall with nprobes later.
    # If you prefer exact baseline first, comment this out and rely on flat scan while testing.
    tbl.create_index(
        column="vector",
        index_type="ivf_pq",          # or "ivf_flat" / "hnsw" if available in your version
        metric="cosine",
        num_partitions=64,            # tune for your data size (e.g., ~sqrt(N))
        num_sub_vectors=16            # PQ granularity; raise for better recall
    )

def lancedb_where_clause(filter_dict: dict | None) -> str | None:
    if not filter_dict:
        return None
    parts = []
    for k, v in filter_dict.items():
        if v is None:
            continue
        # allow list-like values
        if isinstance(v, (list, tuple, set)):
            vv = [f"'{str(x).replace(\"'\", \"''\")}'" for x in v]
            parts.append(f"{k} IN ({', '.join(vv)})")
        else:
            s = str(v).replace("'", "''")
            parts.append(f"{k} = '{s}'")
    return " AND ".join(parts) if parts else None

def search_lancedb(query: str, table, top_k: int = 20, filter_dict: dict | None = None):
    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True)

    # Build search
    sq = table.search(qvec).metric("cosine")

    # Prefilter BEFORE the ANN probe (closer to Qdrant’s integrated filter)
    where = lancedb_where_clause(filter_dict)
    if where:
        sq = sq.where(where)  # LanceDB treats this as prefilter for the vector scan

    # Overfetch similar to your Qdrant loop and refine
    over_k = max(top_k * 3, top_k + 10)   # extra headroom
    sq = sq.limit(over_k).refine_factor(3)   # re-rank a larger candidate set exactly

    # Probe more partitions (higher recall; slower). Tune 8–32.
    sq = sq.nprobes(16)

    df = sq.to_pandas()

    # Clip to final top_k (already ranked by distance)
    return df.head(top_k)

###########################################################
# Qdrant
from qdrant_client.http.models import SearchParams

def dump_into_qdrant(chunks, collection_name="my_docs_bge"):
    client = QdrantClient(path=qdrant_path)
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=768, distance=Distance.COSINE)
    )

    rows = []
    for i, ch in enumerate(chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            **ch["metadata"]
        })

    texts = [r["text"] for r in rows]
    payloads = rows
    vectors = [encode_text(t, normalize=True).tolist() for t in texts]  # normalize on write (optional but consistent)

    ids = [r["id"] for r in rows]

    client.upload_collection(
        collection_name=collection_name,
        vectors=vectors,
        payload=payloads,
        ids=ids,
    )
    return

def search_qdrant(query, client, collection_name="my_docs_bge", top_k=20, stages: list[dict] | None=None):
    if stages is None:
        stages = [{}]

    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True).tolist()

    results = []
    seen_ids = set()
    for stage in stages:
        need = top_k - len(results)
        if need <= 0:
            break
        flt = make_filter(stage)
        hits = client.search(
            collection_name=collection_name,
            query_vector=qvec,
            query_filter=flt,
            limit=max(need*3, need+10),
            with_payload=True,
            params=SearchParams(hnsw_ef=128, exact=False)  # make “default” recall explicit
        )
        for pt in hits:
            if pt.id not in seen_ids:
                results.append(pt)
                seen_ids.add(pt.id)
                if len(results) >= top_k:
                    break
    return results

##############################################################################################

import time
from typing import Dict, Optional, Tuple, List

import lancedb
import pandas as pd
from sentence_transformers import SentenceTransformer

# -----------------------------
# Config
# -----------------------------
DB_PATH = "./lancedb_progressive"
TABLE_NAME = "text_chunks"
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
MAX_RESULTS_DEFAULT = 20

# First-removed -> last-removed (lower number = remove earlier)
FILTER_PRIORITY_DEFAULT = {
    "topic": 1,
    "product": 2,
    "client_type": 3,
    "region": 4,
    "country": 5,
}


def build_filter_string(filters: Dict[str, str]) -> Optional[str]:
    """Build SQL-like filter string from a dict (ignores None/empty)."""
    if not filters:
        return None
    parts = []
    for k, v in filters.items():
        if v is not None and str(v) != "":
            parts.append(f"{k} = '{v}'")
    return " AND ".join(parts) if parts else None

def clean_filters(filters: Dict[str, str]) -> Dict[str, str]:
    """Remove None/empty values from filters."""
    return {k: v for k, v in filters.items() if v is not None and str(v) != ""}

def execute_search(
    table,
    model,
    query_text: str,
    filters: Dict[str, str],
    limit: int = MAX_RESULTS_DEFAULT
) -> Tuple[pd.DataFrame, int]:
    """Run a single semantic search with optional filters."""
    emb = model.encode(query_text)
    fstr = build_filter_string(filters)

    if fstr:
        df = table.search(emb).where(fstr).limit(limit).to_pandas()
    else:
        df = table.search(emb).limit(limit).to_pandas()

    return df, len(df)

# -----------------------------
# Progressive Relaxation (functions)
# -----------------------------

def order_filters_by_priority(
    active_filters: Dict[str, str],
    priority_map: Dict[str, int] = FILTER_PRIORITY_DEFAULT
) -> List[Tuple[str, str]]:
    """Return [(key, value), ...] sorted ascending by priority (remove earlier first)."""
    return sorted(active_filters.items(), key=lambda kv: priority_map.get(kv[0], 999))

def progressive_search(
    table,
    model,
    query_text: str,
    initial_filters: Dict[str, str],
    max_results: int = MAX_RESULTS_DEFAULT,
    filter_priority: Dict[str, int] = FILTER_PRIORITY_DEFAULT,
    verbose: bool = True
) -> Dict:
    """
    Perform progressive filter relaxation until we get results (up to max_results),
    removing filters in priority order.
    """
    active_filters = clean_filters(initial_filters.copy())
    removed: List[str] = []
    iterations = 0

    # 1) Try with all filters
    results, count = execute_search(table, model, query_text, active_filters, limit=max_results)
    iterations += 1

    # If we already have results (or exactly pure similarity with no filters), return
    if count >= max_results or (count > 0 and len(active_filters) == 0):
        return results
    # 2) Relax in priority order
    for key, _ in order_filters_by_priority(active_filters, filter_priority):
        # remove one filter, then search again
        active_filters.pop(key, None)
        removed.append(key)
        iterations += 1

        results, count = execute_search(table, model, query_text, active_filters, limit=max_results)

        if count >= max_results:
            break

        if not active_filters:
            break

    return results

def prettify(df):
    results = []
    for _, row in df.iterrows():
        entry = {
            'text':row['text'],
            'metadata': {
                'region':row['region'],
                'country':row['country'],
                'client_type':row['client_type'],
                'product':row['product'],
                'topic':row['topic'],
                'location':row['location'],
                'filename':row['filename'],
                'updated_date':row['updated_date'],
                'year':row['year'],
            }
        }
        results.append(entry)
    return results

# -----------------------------
# Example usage (run directly)
# -----------------------------
if __name__ == "__main__":

    # query + filters
    query = "price of bangladesh"
    filters = {}

    # run progressive search
    results = progressive_search(
        table=tbl,
        model=model,
        query_text=query,
        initial_filters=filters,
        max_results=20,
        filter_priority=FILTER_PRIORITY_DEFAULT,
        verbose=True,
    )
    # results = prettify(results)
    # pretty print
    results

"""
Integrated Semantic-First Progressive Filter System
Combines metadata extraction with query intent analysis
"""

import lancedb
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import Any, Dict, Optional, List, Tuple, Set
from collections import defaultdict
import time
import re

# Initialize model
model = SentenceTransformer('all-MiniLM-L6-v2')

# ========================================
# METADATA EXTRACTION (Your existing code)
# ========================================

SESSION_STORE = {}

# Alias mappings (extend as needed)
alias_mapping = {
    'SG': 'SINGAPORE',
    'SINGAPORE': 'SINGAPORE',
    'MY': 'MALAYSIA',
    'MALAYSIA': 'MALAYSIA',
    'TH': 'THAILAND',
    'THAILAND': 'THAILAND',
    'ID': 'INDONESIA',
    'INDONESIA': 'INDONESIA',
    'PH': 'PHILIPPINES',
    'PHILIPPINES': 'PHILIPPINES',
    'VN': 'VIETNAM',
    'VIETNAM': 'VIETNAM',
    'BD': 'BANGLADESH',
    'BANGLADESH': 'BANGLADESH',
    'IN': 'INDIA',
    'INDIA': 'INDIA',
    'CN': 'CHINA',
    'CHINA': 'CHINA',
    'JP': 'JAPAN',
    'JAPAN': 'JAPAN',
    'KR': 'KOREA',
    'KOREA': 'KOREA',
    'US': 'USA',
    'USA': 'USA',
    'UNITED STATES': 'USA',
    'UK': 'UK',
    'UNITED KINGDOM': 'UK',
    'DE': 'GERMANY',
    'GERMANY': 'GERMANY',
    'FR': 'FRANCE',
    'FRANCE': 'FRANCE',
    'AU': 'AUSTRALIA',
    'AUSTRALIA': 'AUSTRALIA',
}

# Regional groupings
compiled_region_dict = {
    'ASIA PACIFIC': ['SINGAPORE', 'MALAYSIA', 'THAILAND', 'INDONESIA', 'PHILIPPINES', 
                     'VIETNAM', 'BANGLADESH', 'INDIA', 'CHINA', 'JAPAN', 'KOREA', 'AUSTRALIA'],
    'APAC': ['SINGAPORE', 'MALAYSIA', 'THAILAND', 'INDONESIA', 'PHILIPPINES',
             'VIETNAM', 'BANGLADESH', 'INDIA', 'CHINA', 'JAPAN', 'KOREA', 'AUSTRALIA'],
    'SOUTHEAST ASIA': ['SINGAPORE', 'MALAYSIA', 'THAILAND', 'INDONESIA', 'PHILIPPINES', 'VIETNAM'],
    'SEA': ['SINGAPORE', 'MALAYSIA', 'THAILAND', 'INDONESIA', 'PHILIPPINES', 'VIETNAM'],
    'SOUTH ASIA': ['BANGLADESH', 'INDIA', 'PAKISTAN', 'SRI LANKA'],
    'EAST ASIA': ['CHINA', 'JAPAN', 'KOREA'],
}

def init_filter_dict():
    return dict()

def get_filter_dict(session_id: str):
    if session_id not in SESSION_STORE:
        SESSION_STORE[session_id] = init_filter_dict()
    return SESSION_STORE[session_id]

def reset_filter_dict(session_id: str):
    SESSION_STORE[session_id] = init_filter_dict()

def extend_unique(fd, key, values):
    if not values:
        return
    current = values  # replace the value completely
    fd[key] = list(current)

def map_question_to_countries(question):
    found = set()
    q_upper = question.upper()
    for alias, proper_name in alias_mapping.items():
        if re.search(rf"\b{re.escape(alias)}\b", q_upper):
            found.add(proper_name)
    return list(found)

def map_question_to_region(question):
    found = set()
    q_upper = question.upper()
    regions = ['ASIA', 'AME', 'EMEA', 'EUROPE', 'AMERICAS', 'AFRICA']
    for region in regions:
        if re.search(rf"\b{re.escape(region)}\b", q_upper):
            found.add(region)
    return list(found)

def map_unofficialregion_to_countries(question):
    found = set()
    q_upper = question.upper()
    for region_alias, country_listing in compiled_region_dict.items():
        if re.search(rf"\b{re.escape(region_alias.upper())}\b", q_upper):
            found.update(country_listing)
    return list(found)

def map_question_to_product(question):
    found = set()
    q_upper = question.upper()
    products = ['ALL', 'CASH', 'TRADE', 'FX', 'SECURITIES', 'TREASURY']
    for product in products:
        if re.search(rf"\b{re.escape(product)}\b", q_upper):
            found.add(product)
    return list(found)

def map_question_to_client_type(question):
    found = set()
    q_upper = question.upper()
    client_types = ['ALL', 'CORPORATES', 'FI', 'FINANCIAL INSTITUTION', 'PAYTECH', 'RETAIL', 'INSTITUTIONAL']
    
    # Map variations to standard terms
    client_map = {
        'FINANCIAL INSTITUTION': 'FI',
        'FI': 'FI',
        'CORPORATES': 'CORPORATES',
        'CORPORATE': 'CORPORATES',
        'RETAIL': 'RETAIL',
        'INSTITUTIONAL': 'INSTITUTIONAL',
        'PAYTECH': 'PAYTECH'
    }
    
    for client_type in client_types:
        if re.search(rf"\b{re.escape(client_type)}\b", q_upper):
            standardized = client_map.get(client_type, client_type)
            found.add(standardized)
    return list(found)

def map_question_to_topic(question):
    """Extended to include more topics beyond S2B"""
    found = set()
    q_upper = question.upper()
    
    # Topic variations and their standard forms
    topic_patterns = {
        'S2B': ['S2B', 'STRAIGHT2BANK', 'STRAIGHT 2 BANK'],
        'ACH': ['ACH', 'AUTOMATED CLEARING HOUSE'],
        'RTGS': ['RTGS', 'REAL TIME GROSS SETTLEMENT'],
        'SWIFT': ['SWIFT'],
        'WIRE': ['WIRE', 'WIRE TRANSFER'],
        'SEPA': ['SEPA'],
        'FAST': ['FAST', 'FAST PAYMENT'],
        'GIRO': ['GIRO'],
        'MEPS': ['MEPS'],
    }
    
    for standard_topic, variations in topic_patterns.items():
        for variant in variations:
            if re.search(rf"\b{re.escape(variant)}\b", q_upper):
                found.add(standard_topic)
                break
    
    return list(found)

def process_question(session_id: str, question: str):
    """Extract metadata filters from question"""
    fd = get_filter_dict(session_id)
    
    extracted_countries = map_question_to_countries(question)
    extracted_region = map_question_to_region(question)
    extracted_indirect_countries = map_unofficialregion_to_countries(question)
    extracted_products = map_question_to_product(question)
    extracted_client_types = map_question_to_client_type(question)
    extracted_topics = map_question_to_topic(question)
    
    # If both region and countries are extracted, skip indirect countries
    if extracted_region and extracted_countries:
        extracted_indirect_countries = []
    
    # Augment question with indirect countries for better semantic search
    if extracted_indirect_countries:
        question += ", " + ", ".join(extracted_indirect_countries)
    
    # Combine all countries
    parsed_country = list(set(extracted_countries + extracted_indirect_countries))
    
    # Update filter dictionary
    extend_unique(fd, "country", parsed_country)
    extend_unique(fd, "region", extracted_region)
    extend_unique(fd, "product", extracted_products)
    extend_unique(fd, "client_type", extracted_client_types)
    extend_unique(fd, "topic", extracted_topics)
    
    return fd, question

# ========================================
# ENHANCED QUERY INTENT ANALYZER
# ========================================

class EnhancedQueryIntentAnalyzer:
    """
    Analyzes query intent for negations and contradictions
    Integrates with metadata extraction
    """
    
    def __init__(self):
        # Negative patterns
        self.negative_patterns = [
            r'without\s+(\w+)',
            r'exclude\s+(\w+)',
            r'excluding\s+(\w+)',
            r'except\s+(\w+)',
            r'not\s+(?:in\s+)?(\w+)',
            r'no\s+(\w+)',
            r'avoid\s+(\w+)',
            r'skip\s+(\w+)',
            r'minus\s+(\w+)',
            r'remove\s+(\w+)',
            r'omit\s+(\w+)',
        ]
        
        # Multi-word negative patterns
        self.multi_word_negative_patterns = [
            r'without\s+([\w\s]+?)(?:\.|,|;|$)',
            r'excluding\s+([\w\s]+?)(?:\.|,|;|$)',
            r'except\s+(?:for\s+)?([\w\s]+?)(?:\.|,|;|$)',
        ]
    
    def extract_negations_enhanced(self, question: str) -> Dict[str, Set[str]]:
        """
        Extract negations using both simple and complex patterns
        Returns categorized negations
        """
        q_lower = question.lower()
        negations = {
            'countries': set(),
            'regions': set(),
            'products': set(),
            'topics': set(),
            'client_types': set(),
            'general': set()
        }
        
        # Extract using simple patterns
        for pattern in self.negative_patterns:
            matches = re.findall(pattern, q_lower)
            for match in matches:
                categorized = self._categorize_term(match)
                for category, terms in categorized.items():
                    negations[category].update(terms)
        
        # Extract using multi-word patterns
        for pattern in self.multi_word_negative_patterns:
            matches = re.findall(pattern, q_lower)
            for match in matches:
                # Split multi-word matches and categorize each
                words = match.strip().split()
                for word in words:
                    categorized = self._categorize_term(word)
                    for category, terms in categorized.items():
                        negations[category].update(terms)
        
        # Convert sets to lists
        return {k: list(v) for k, v in negations.items()}
    
    def _categorize_term(self, term: str) -> Dict[str, Set[str]]:
        """Categorize a term into appropriate metadata category"""
        term = term.strip().upper()
        result = defaultdict(set)
        
        # Check if it's a country
        if term in alias_mapping or term in alias_mapping.values():
            proper_name = alias_mapping.get(term, term)
            result['countries'].add(proper_name)
        
        # Check if it's a region
        regions = {'ASIA', 'EUROPE', 'AMERICAS', 'AFRICA', 'AME', 'EMEA', 'APAC'}
        if term in regions:
            result['regions'].add(term)
        
        # Check if it's a product
        products = {'CASH', 'TRADE', 'FX', 'SECURITIES', 'TREASURY'}
        if term in products:
            result['products'].add(term)
        
        # Check if it's a topic
        topics = {'S2B', 'ACH', 'RTGS', 'SWIFT', 'WIRE', 'SEPA', 'FAST', 'GIRO', 'MEPS'}
        if term in topics:
            result['topics'].add(term)
        
        # Check if it's a client type
        client_types = {'CORPORATES', 'FI', 'RETAIL', 'INSTITUTIONAL', 'PAYTECH'}
        if term in client_types:
            result['client_types'].add(term)
        
        # If no category matched, add to general
        if not result:
            result['general'].add(term)
        
        return result
    
    def detect_contradictions(self, extracted_filters: Dict, negations: Dict) -> Dict[str, List[str]]:
        """
        Detect contradictions between extracted filters and negations
        Returns filters that should be overridden
        """
        contradictions = {}
        
        # Check country contradictions
        if 'country' in extracted_filters and negations.get('countries'):
            filter_countries = set(extracted_filters['country'])
            negated_countries = set(negations['countries'])
            conflicts = filter_countries.intersection(negated_countries)
            if conflicts:
                contradictions['country'] = list(conflicts)
        
        # Check region contradictions
        if 'region' in extracted_filters and negations.get('regions'):
            filter_regions = set(extracted_filters['region'])
            negated_regions = set(negations['regions'])
            conflicts = filter_regions.intersection(negated_regions)
            if conflicts:
                contradictions['region'] = list(conflicts)
        
        # Check other metadata contradictions
        for key in ['product', 'topic', 'client_type']:
            if key in extracted_filters and negations.get(f'{key}s'):
                filter_values = set(extracted_filters[key])
                negated_values = set(negations[f'{key}s'])
                conflicts = filter_values.intersection(negated_values)
                if conflicts:
                    contradictions[key] = list(conflicts)
        
        return contradictions

# ========================================
# INTEGRATED SEMANTIC SEARCH
# ========================================

def integrated_semantic_search(
    table,
    model,
    session_id: str,
    query_text: str,
    override_filters: Dict[str, Any] = None,
    max_results: int = 20,
    semantic_threshold: float = 0.7,
    verbose: bool = True
) -> Dict:
    """
    Complete integrated search with metadata extraction and semantic priority
    
    Process:
    1. Extract metadata filters from query
    2. Detect negations and contradictions
    3. Adjust filters based on query intent
    4. Perform semantic-priority progressive search
    """
    
    start_time = time.time()
    analyzer = EnhancedQueryIntentAnalyzer()
    
    # Step 1: Extract metadata filters
    extracted_filters, augmented_query = process_question(session_id, query_text)
    
    # Allow manual override of filters
    if override_filters:
        extracted_filters.update(override_filters)
    
    # Step 2: Extract negations from query
    negations = analyzer.extract_negations_enhanced(query_text)
    
    # Step 3: Detect contradictions
    contradictions = analyzer.detect_contradictions(extracted_filters, negations)
    
    # Step 4: Adjust filters based on contradictions
    adjusted_filters = {}
    removed_filters = {}
    
    for key, values in extracted_filters.items():
        if key in contradictions:
            # This filter contradicts the query intent
            removed_filters[key] = values
            if verbose:
                print(f"⚠️ Removing contradicting filter: {key}={values}")
                print(f"   Reason: Query contains negation for {contradictions[key]}")
        else:
            adjusted_filters[key] = values
    
    if verbose:
        print("\n" + "="*60)
        print("🔍 INTEGRATED SEMANTIC SEARCH")
        print("="*60)
        print(f"Original query: '{query_text}'")
        print(f"Augmented query: '{augmented_query}'")
        print(f"Extracted filters: {extracted_filters}")
        print(f"Detected negations: {negations}")
        print(f"Contradictions found: {contradictions}")
        print(f"Adjusted filters: {adjusted_filters}")
        print("-"*60)
    
    # Step 5: Perform semantic-priority search with progressive relaxation
    results = progressive_semantic_search_integrated(
        table=table,
        model=model,
        query_text=augmented_query,
        initial_filters=adjusted_filters,
        negations=negations,
        max_results=max_results,
        semantic_threshold=semantic_threshold,
        verbose=verbose
    )
    
    # Add metadata to results
    results['original_query'] = query_text
    results['augmented_query'] = augmented_query
    results['extracted_filters'] = extracted_filters
    results['contradictions'] = contradictions
    results['negations'] = negations
    results['execution_time'] = time.time() - start_time
    
    return results

def progressive_semantic_search_integrated(
    table,
    model,
    query_text: str,
    initial_filters: Dict[str, Any],
    negations: Dict[str, List[str]],
    max_results: int = 20,
    semantic_threshold: float = 0.7,
    verbose: bool = True
) -> Dict:
    """
    Progressive search with semantic priority and negation handling
    """
    
    # Filter priority
    filter_priority = {
        "topic": 1,
        "product": 2,
        "client_type": 3,
        "region": 4,
        "country": 5
    }
    
    active_filters = initial_filters.copy()
    removed_filters = []
    iterations = 0
    
    while True:
        iterations += 1
        
        # Perform semantic search with negation penalties
        results_df, count = semantic_search_with_negations(
            table, model, query_text, active_filters, negations,
            limit=max_results, semantic_threshold=semantic_threshold
        )
        
        if verbose:
            print(f"\n📊 Iteration {iterations}:")
            print(f"   Active filters: {list(active_filters.keys()) if active_filters else 'None'}")
            print(f"   Results found: {count}")
        
        # Check if we have enough results
        if count >= max_results or not active_filters:
            if verbose:
                if count >= max_results:
                    print(f"✅ Target reached with {count} results")
                else:
                    print(f"⚠️ All filters removed. Found {count} results")
            break
        
        # Remove lowest priority filter
        if active_filters:
            filters_by_priority = sorted(
                active_filters.items(),
                key=lambda x: filter_priority.get(x[0], 999)
            )
            
            if filters_by_priority:
                key_to_remove, value_removed = filters_by_priority[0]
                del active_filters[key_to_remove]
                removed_filters.append({'key': key_to_remove, 'value': value_removed})
                
                if verbose:
                    print(f"   → Removing filter: {key_to_remove}")
    
    return {
        'results': results_df,
        'applied_filters': active_filters,
        'removed_filters': removed_filters,
        'iterations': iterations
    }

def semantic_search_with_negations(
    table, model, query_text: str, filters: Dict,
    negations: Dict, limit: int, semantic_threshold: float
) -> Tuple[pd.DataFrame, int]:
    """
    Semantic search with negation penalties
    """
    
    # Encode query
    qvec = model.encode(query_text)
    qvec = qvec / np.linalg.norm(qvec)
    
    # Get candidates (more than needed for re-ranking)
    candidates_df = (
        table.search(qvec)
        .metric("cosine")
        .limit(limit * 5)
        .to_pandas()
    )
    
    if len(candidates_df) == 0:
        return pd.DataFrame(), 0
    
    # Calculate semantic scores
    candidates_df['semantic_score'] = 1 - candidates_df['_distance']
    
    # Apply filter scoring
    candidates_df['filter_score'] = 1.0
    
    # Boost matches with filters
    for key, values in filters.items():
        if values and key in candidates_df.columns:
            if isinstance(values, list):
                matches = candidates_df[key].isin(values)
            else:
                matches = candidates_df[key] == values
            
            candidates_df.loc[matches, 'filter_score'] *= 1.2
            candidates_df.loc[~matches, 'filter_score'] *= 0.9
    
    # Apply negation penalties
    if 'countries' in negations:
        for country in negations['countries']:
            if 'country' in candidates_df.columns:
                mask = candidates_df['country'].str.upper() == country.upper()
                candidates_df.loc[mask, 'filter_score'] *= 0.2  # Heavy penalty
    
    if 'regions' in negations:
        for region in negations['regions']:
            if 'region' in candidates_df.columns:
                mask = candidates_df['region'].str.upper() == region.upper()
                candidates_df.loc[mask, 'filter_score'] *= 0.2
    
    # Combine scores
    candidates_df['final_score'] = (
        candidates_df['semantic_score'] * semantic_threshold +
        candidates_df['filter_score'] * (1 - semantic_threshold)
    )
    
    # Sort and limit
    results_df = candidates_df.nlargest(limit, 'final_score')
    
    return results_df, len(results_df)

# ========================================
# HELPER FUNCTIONS
# ========================================

def build_filter_string(filters: Dict[str, Any]) -> Optional[str]:
    """Build SQL filter string (for fallback)"""
    if not filters:
        return None
    
    clauses = []
    for key, values in filters.items():
        if not values:
            continue
        
        if isinstance(values, list):
            if len(values) == 1:
                clauses.append(f"{key} = '{values[0]}'")
            else:
                values_str = "', '".join(values)
                clauses.append(f"{key} IN ('{values_str}')")
        else:
            clauses.append(f"{key} = '{values}'")
    
    return " AND ".join(clauses) if clauses else None

# ========================================
# USAGE EXAMPLE
# ========================================

def main():
    """Example usage of integrated system"""
    
    # Setup test database
    db = lancedb.connect("./lancedb_integrated")
    
    # Sample documents
    documents = [
        {
            'text': 'S2B payment services available in Singapore for corporate clients',
            'metadata': {
                'region': 'ASIA', 'country': 'SINGAPORE', 'topic': 'S2B',
                'product': 'CASH', 'client_type': 'CORPORATES'
            }
        },
        {
            'text': 'S2B implementation for all Asian countries including Bangladesh',
            'metadata': {
                'region': 'ASIA', 'country': 'BANGLADESH', 'topic': 'S2B',
                'product': 'CASH', 'client_type': 'CORPORATES'
            }
        },
        {
            'text': 'ACH payment system in Malaysia for retail banking',
            'metadata': {
                'region': 'ASIA', 'country': 'MALAYSIA', 'topic': 'ACH',
                'product': 'CASH', 'client_type': 'RETAIL'
            }
        },
        {
            'text': 'S2B services for APAC region excluding Bangladesh and Myanmar',
            'metadata': {
                'region': 'ASIA', 'country': 'ALL', 'topic': 'S2B',
                'product': 'CASH', 'client_type': 'CORPORATES'
            }
        }
    ]
    
    # Create table
    data = []
    for i, doc in enumerate(documents):
        vec = model.encode(doc['text'])
        vec = vec / np.linalg.norm(vec)
        record = {
            'id': i,
            'text': doc['text'],
            'vector': vec,
            **doc['metadata']
        }
        data.append(record)
    
    if 'documents' in db.table_names():
        db.drop_table('documents')
    
    table = db.create_table('documents', pd.DataFrame(data))
    print(f"✅ Created table with {len(data)} documents")
    
    # Test cases
    print("\n" + "🚀"*30)
    print("TEST 1: Query with negation")
    print("🚀"*30)
    
    session_id = "test_session_1"
    query1 = "S2B payment capability without Bangladesh for corporates"
    
    result1 = integrated_semantic_search(
        table=table,
        model=model,
        session_id=session_id,
        query_text=query1,
        max_results=5,
        semantic_threshold=0.7,
        verbose=True
    )
    
    print("\n📄 Results:")
    if 'results' in result1 and not result1['results'].empty:
        for idx, row in result1['results'].head(3).iterrows():
            print(f"\n  Result {idx+1}:")
            print(f"    Text: {row['text'][:80]}...")
            print(f"    Country: {row['country']}")
            if 'final_score' in row:
                print(f"    Score: {row['final_score']:.3f}")
    
    print("\n" + "🚀"*30)
    print("TEST 2: Query with region expansion")
    print("🚀"*30)
    
    session_id = "test_session_2"
    query2 = "ACH services in APAC excluding Bangladesh"
    
    result2 = integrated_semantic_search(
        table=table,
        model=model,
        session_id=session_id,
        query_text=query2,
        max_results=5,
        verbose=True
    )

if __name__ == "__main__":
    main()

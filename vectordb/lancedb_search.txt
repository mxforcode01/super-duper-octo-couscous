EMB_MODEL = SentenceTransformer(embedding_path)  # single source of truth

def encode_text(text: str, normalize: bool = False):
    v = EMB_MODEL.encode(text)
    if normalize:
        # cosine is scale-invariant if both sides normalized; keep it consistent
        import numpy as np
        n = np.linalg.norm(v)
        if n > 0:
            v = v / n
    return v

def make_query_text(q: str) -> str:
    return "query: " + str(q)  # matches your Qdrant behavior

def dump_into_lancedb(text_chunks, db_path, table_name):
    db = lancedb.connect(db_path)
    if table_name in db.table_names():
        db.drop_table(table_name)

    rows = []
    for i, ch in enumerate(text_chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            "vector": encode_text(ch["text"], normalize=True),  # normalize on write
            **ch["metadata"]
        })
    df = pd.DataFrame(rows)
    tbl = db.create_table(table_name, df)

    # Build an ANN index with cosine metric.
    # If your dataset is small (<~100k), IVF_PQ is still fine; bump recall with nprobes later.
    # If you prefer exact baseline first, comment this out and rely on flat scan while testing.
    tbl.create_index(
        column="vector",
        index_type="ivf_pq",          # or "ivf_flat" / "hnsw" if available in your version
        metric="cosine",
        num_partitions=64,            # tune for your data size (e.g., ~sqrt(N))
        num_sub_vectors=16            # PQ granularity; raise for better recall
    )

def lancedb_where_clause(filter_dict: dict | None) -> str | None:
    if not filter_dict:
        return None
    parts = []
    for k, v in filter_dict.items():
        if v is None:
            continue
        # allow list-like values
        if isinstance(v, (list, tuple, set)):
            vv = [f"'{str(x).replace(\"'\", \"''\")}'" for x in v]
            parts.append(f"{k} IN ({', '.join(vv)})")
        else:
            s = str(v).replace("'", "''")
            parts.append(f"{k} = '{s}'")
    return " AND ".join(parts) if parts else None

def search_lancedb(query: str, table, top_k: int = 20, filter_dict: dict | None = None):
    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True)

    # Build search
    sq = table.search(qvec).metric("cosine")

    # Prefilter BEFORE the ANN probe (closer to Qdrant’s integrated filter)
    where = lancedb_where_clause(filter_dict)
    if where:
        sq = sq.where(where)  # LanceDB treats this as prefilter for the vector scan

    # Overfetch similar to your Qdrant loop and refine
    over_k = max(top_k * 3, top_k + 10)   # extra headroom
    sq = sq.limit(over_k).refine_factor(3)   # re-rank a larger candidate set exactly

    # Probe more partitions (higher recall; slower). Tune 8–32.
    sq = sq.nprobes(16)

    df = sq.to_pandas()

    # Clip to final top_k (already ranked by distance)
    return df.head(top_k)

###########################################################
# Qdrant
from qdrant_client.http.models import SearchParams

def dump_into_qdrant(chunks, collection_name="my_docs_bge"):
    client = QdrantClient(path=qdrant_path)
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=768, distance=Distance.COSINE)
    )

    rows = []
    for i, ch in enumerate(chunks):
        rows.append({
            "id": i,
            "text": ch["text"],
            **ch["metadata"]
        })

    texts = [r["text"] for r in rows]
    payloads = rows
    vectors = [encode_text(t, normalize=True).tolist() for t in texts]  # normalize on write (optional but consistent)

    ids = [r["id"] for r in rows]

    client.upload_collection(
        collection_name=collection_name,
        vectors=vectors,
        payload=payloads,
        ids=ids,
    )
    return

def search_qdrant(query, client, collection_name="my_docs_bge", top_k=20, stages: list[dict] | None=None):
    if stages is None:
        stages = [{}]

    qtxt = make_query_text(query)
    qvec = encode_text(qtxt, normalize=True).tolist()

    results = []
    seen_ids = set()
    for stage in stages:
        need = top_k - len(results)
        if need <= 0:
            break
        flt = make_filter(stage)
        hits = client.search(
            collection_name=collection_name,
            query_vector=qvec,
            query_filter=flt,
            limit=max(need*3, need+10),
            with_payload=True,
            params=SearchParams(hnsw_ef=128, exact=False)  # make “default” recall explicit
        )
        for pt in hits:
            if pt.id not in seen_ids:
                results.append(pt)
                seen_ids.add(pt.id)
                if len(results) >= top_k:
                    break
    return results

##############################################################################################

import numpy as np

from typing import Any, Dict, Optional

def _is_all(v: Any) -> bool:
    return isinstance(v, str) and v.strip().upper() == "ALL"

def _sql_quote(v: Any) -> str:
    # Quote strings safely (double single quotes); pass numbers/bools through.
    if isinstance(v, str):
        return "'" + v.replace("'", "''") + "'"
    if isinstance(v, bool):
        return "TRUE" if v else "FALSE"
    return str(v)

def build_filter_string(filters: Dict[str, Any]) -> Optional[str]:
    """
    Build a LanceDB SQL-like WHERE clause from dict values.
    Supports scalars and sequences. If any value in a key's list is 'ALL',
    that key is skipped (no filtering on that field).
    Examples:
      {'country': ['SINGAPORE','MALAYSIA'], 'product':['CASH']}
         -> "country IN ('SINGAPORE','MALAYSIA') AND product = 'CASH'"
    """
    if not filters:
        return None

    clauses = []
    for key, raw in filters.items():
        if raw is None:
            continue

        # Normalize to a list for uniform handling
        if isinstance(raw, (list, tuple, set)):
            values = [v for v in raw if v is not None and (not isinstance(v, str) or v.strip() != "")]
        else:
            values = [raw]

        if not values:
            continue

        # If ANY value is 'ALL', skip this key entirely (no filter on it)
        if any(_is_all(v) for v in values):
            continue

        # De-dupe while preserving order (case-insensitive for strings)
        seen = set()
        deduped = []
        for v in values:
            key_v = v.upper() if isinstance(v, str) else v
            if key_v in seen:
                continue
            seen.add(key_v)
            deduped.append(v)

        # Emit = or IN (...)
        if len(deduped) == 1:
            clauses.append(f"{key} = {_sql_quote(deduped[0])}")
        else:
            joined = ", ".join(_sql_quote(v) for v in deduped)
            clauses.append(f"{key} IN ({joined})")

    return " AND ".join(clauses) if clauses else None


def execute_search(
    table,
    model,
    query_text: str,
    filters: Dict[str, object],
    limit: int = 20,
    overfetch_mult: int = 3,
    refine_factor: int = 3,
    nprobes: int = 16,
) -> Tuple[pd.DataFrame, int]:
    """
    Qdrant-like LanceDB search:
    - cosine metric
    - normalized query vector
    - prefilter via .where(...)
    - overfetch + refine + nprobes to raise recall
    """
    qtxt = make_query_text(query_text)
    qvec = encode_query(model, qtxt, normalize=True)

    # Drop any 'ALL' values to match your usual semantics
    filters = drop_all_values(filters)
    fstr = build_filter_string(filters)

    # Overfetch similar to your Qdrant loop (need * 3)
    over_k = max(limit * overfetch_mult, limit + 10)

    sq = table.search(qvec).metric("cosine")
    if fstr:
        sq = sq.where(fstr)           # PREFILTER: affect the candidate set
    sq = sq.nprobes(nprobes)          # probe more partitions (higher recall)
    sq = sq.refine_factor(refine_factor)  # re-rank a larger candidate set exactly
    sq = sq.limit(over_k)             # over-fetch then clip

    df = sq.to_pandas()
    # NOTE: LanceDB returns already sorted results (by distance/score).
    # Clip to requested limit:
    df = df.head(limit)
    return df, len(df)

FILTER_PRIORITY_DEFAULT = {
    "topic": 1,
    "product": 2,
    "client_type": 3,
    "region": 4,
    "country": 5,
}

def clean_filters(filters: Dict[str, str]) -> Dict[str, str]:
    return {k: v for k, v in (filters or {}).items() if v is not None and str(v) != ""}

def order_filters_by_priority(
    active_filters: Dict[str, str],
    priority_map: Dict[str, int] = FILTER_PRIORITY_DEFAULT
) -> List[Tuple[str, str]]:
    return sorted(active_filters.items(), key=lambda kv: priority_map.get(kv[0], 999))

def progressive_search(
    table,
    model,
    query_text: str,
    initial_filters: Dict[str, str],
    max_results: int = 20,
    filter_priority: Dict[str, int] = FILTER_PRIORITY_DEFAULT,
    verbose: bool = True
) -> pd.DataFrame:
    active_filters = drop_all_values(clean_filters(initial_filters.copy()))
    iterations = 0

    # 1) Try with all filters
    df, count = execute_search(table, model, query_text, active_filters, limit=max_results)
    iterations += 1
    if count >= max_results or (count > 0 and len(active_filters) == 0):
        return df

    # 2) Relax in priority order (topic → product → client_type → region → country)
    for key, _ in order_filters_by_priority(active_filters, filter_priority):
        active_filters.pop(key, None)
        iterations += 1
        df, count = execute_search(table, model, query_text, active_filters, limit=max_results)
        if count >= max_results or not active_filters:
            break

    return df


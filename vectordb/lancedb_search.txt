"""
Corrected Semantic-First Progressive Filter System
Respects extracted metadata filters and only adjusts for negations
"""

import lancedb
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import Any, Dict, Optional, List, Tuple, Set
from collections import defaultdict
import time
import re

# Initialize model
model = SentenceTransformer('all-MiniLM-L6-v2')

# ========================================
# METADATA EXTRACTION (Your existing code)
# ========================================

SESSION_STORE = {}

# Alias mappings (extend as needed)
alias_mapping = {
    'SG': 'SINGAPORE',
    'SINGAPORE': 'SINGAPORE',
    'MY': 'MALAYSIA',
    'MALAYSIA': 'MALAYSIA',
    'TH': 'THAILAND',
    'THAILAND': 'THAILAND',
    'ID': 'INDONESIA',
    'INDONESIA': 'INDONESIA',
    'PH': 'PHILIPPINES',
    'PHILIPPINES': 'PHILIPPINES',
    'VN': 'VIETNAM',
    'VIETNAM': 'VIETNAM',
    'BD': 'BANGLADESH',
    'BANGLADESH': 'BANGLADESH',
    'IN': 'INDIA',
    'INDIA': 'INDIA',
    'CN': 'CHINA',
    'CHINA': 'CHINA',
    'JP': 'JAPAN',
    'JAPAN': 'JAPAN',
    'KR': 'KOREA',
    'KOREA': 'KOREA',
    'US': 'USA',
    'USA': 'USA',
    'UNITED STATES': 'USA',
    'UK': 'UK',
    'UNITED KINGDOM': 'UK',
    'DE': 'GERMANY',
    'GERMANY': 'GERMANY',
    'FR': 'FRANCE',
    'FRANCE': 'FRANCE',
    'AU': 'AUSTRALIA',
    'AUSTRALIA': 'AUSTRALIA',
}

# Regional groupings
compiled_region_dict = {
    'ASIA PACIFIC': ['SINGAPORE', 'MALAYSIA', 'THAILAND', 'INDONESIA', 'PHILIPPINES', 
                     'VIETNAM', 'BANGLADESH', 'INDIA', 'CHINA', 'JAPAN', 'KOREA', 'AUSTRALIA'],
    'APAC': ['SINGAPORE', 'MALAYSIA', 'THAILAND', 'INDONESIA', 'PHILIPPINES',
             'VIETNAM', 'BANGLADESH', 'INDIA', 'CHINA', 'JAPAN', 'KOREA', 'AUSTRALIA'],
    'SOUTHEAST ASIA': ['SINGAPORE', 'MALAYSIA', 'THAILAND', 'INDONESIA', 'PHILIPPINES', 'VIETNAM'],
    'SEA': ['SINGAPORE', 'MALAYSIA', 'THAILAND', 'INDONESIA', 'PHILIPPINES', 'VIETNAM'],
    'SOUTH ASIA': ['BANGLADESH', 'INDIA', 'PAKISTAN', 'SRI LANKA'],
    'EAST ASIA': ['CHINA', 'JAPAN', 'KOREA'],
}

def init_filter_dict():
    return dict()

def get_filter_dict(session_id: str):
    if session_id not in SESSION_STORE:
        SESSION_STORE[session_id] = init_filter_dict()
    return SESSION_STORE[session_id]

def reset_filter_dict(session_id: str):
    SESSION_STORE[session_id] = init_filter_dict()

def extend_unique(fd, key, values):
    if not values:
        return
    current = values  # replace the value completely
    fd[key] = list(current)

def map_question_to_countries(question):
    found = set()
    q_upper = question.upper()
    for alias, proper_name in alias_mapping.items():
        if re.search(rf"\b{re.escape(alias)}\b", q_upper):
            found.add(proper_name)
    return list(found)

def map_question_to_region(question):
    found = set()
    q_upper = question.upper()
    regions = ['ASIA', 'AME', 'EMEA', 'EUROPE', 'AMERICAS', 'AFRICA']
    for region in regions:
        if re.search(rf"\b{re.escape(region)}\b", q_upper):
            found.add(region)
    return list(found)

def map_unofficialregion_to_countries(question):
    found = set()
    q_upper = question.upper()
    for region_alias, country_listing in compiled_region_dict.items():
        if re.search(rf"\b{re.escape(region_alias.upper())}\b", q_upper):
            found.update(country_listing)
    return list(found)

def map_question_to_product(question):
    found = set()
    q_upper = question.upper()
    products = ['ALL', 'CASH', 'TRADE', 'FX', 'SECURITIES', 'TREASURY']
    for product in products:
        if re.search(rf"\b{re.escape(product)}\b", q_upper):
            found.add(product)
    return list(found)

def map_question_to_client_type(question):
    found = set()
    q_upper = question.upper()
    client_types = ['ALL', 'CORPORATES', 'FI', 'FINANCIAL INSTITUTION', 'PAYTECH', 'RETAIL', 'INSTITUTIONAL']
    
    client_map = {
        'FINANCIAL INSTITUTION': 'FI',
        'FI': 'FI',
        'CORPORATES': 'CORPORATES',
        'CORPORATE': 'CORPORATES',
        'RETAIL': 'RETAIL',
        'INSTITUTIONAL': 'INSTITUTIONAL',
        'PAYTECH': 'PAYTECH'
    }
    
    for client_type in client_types:
        if re.search(rf"\b{re.escape(client_type)}\b", q_upper):
            standardized = client_map.get(client_type, client_type)
            found.add(standardized)
    return list(found)

def map_question_to_topic(question):
    found = set()
    q_upper = question.upper()
    
    topic_patterns = {
        'S2B': ['S2B', 'STRAIGHT2BANK', 'STRAIGHT 2 BANK'],
        'ACH': ['ACH', 'AUTOMATED CLEARING HOUSE'],
        'RTGS': ['RTGS', 'REAL TIME GROSS SETTLEMENT'],
        'SWIFT': ['SWIFT'],
        'WIRE': ['WIRE', 'WIRE TRANSFER'],
        'SEPA': ['SEPA'],
        'FAST': ['FAST', 'FAST PAYMENT'],
        'GIRO': ['GIRO'],
        'MEPS': ['MEPS'],
    }
    
    for standard_topic, variations in topic_patterns.items():
        for variant in variations:
            if re.search(rf"\b{re.escape(variant)}\b", q_upper):
                found.add(standard_topic)
                break
    
    return list(found)

def process_question(session_id: str, question: str):
    """
    Extract metadata filters from question
    CORRECTED: No query augmentation, respect extracted filters as final
    """
    fd = get_filter_dict(session_id)
    
    # Extract direct mentions
    extracted_countries = map_question_to_countries(question)
    extracted_region = map_question_to_region(question)
    extracted_products = map_question_to_product(question)
    extracted_client_types = map_question_to_client_type(question)
    extracted_topics = map_question_to_topic(question)
    
    # Handle regional expansions ONLY if no specific countries are mentioned
    extracted_indirect_countries = []
    if not extracted_countries:  # Only expand regions if no specific countries
        extracted_indirect_countries = map_unofficialregion_to_countries(question)
    
    # Combine countries (but only if needed)
    parsed_country = extracted_countries if extracted_countries else extracted_indirect_countries
    
    # Update filter dictionary with ONLY what was found
    if parsed_country:
        extend_unique(fd, "country", parsed_country)
    if extracted_region:
        extend_unique(fd, "region", extracted_region)
    if extracted_products:
        extend_unique(fd, "product", extracted_products)
    if extracted_client_types:
        extend_unique(fd, "client_type", extracted_client_types)
    if extracted_topics:
        extend_unique(fd, "topic", extracted_topics)
    
    # DON'T augment the query - keep it original
    return fd, question

# ========================================
# QUERY NEGATION ANALYZER
# ========================================

class NegationAnalyzer:
    """
    Analyzes query for negations and exclusions
    """
    
    def __init__(self):
        # Patterns for detecting negations
        self.negative_patterns = [
            (r'without\s+([\w\s,]+?)(?:\s+for|\s+in|\.|,|;|$)', 'without'),
            (r'exclude\s+([\w\s,]+?)(?:\s+from|\.|,|;|$)', 'exclude'),
            (r'excluding\s+([\w\s,]+?)(?:\s+from|\.|,|;|$)', 'excluding'),
            (r'except\s+(?:for\s+)?([\w\s,]+?)(?:\.|,|;|$)', 'except'),
            (r'not\s+(?:in\s+)?([\w\s,]+?)(?:\.|,|;|$)', 'not'),
            (r'no\s+([\w\s,]+?)(?:\s+support|\s+service|\.|,|;|$)', 'no'),
            (r'avoid\s+([\w\s,]+?)(?:\.|,|;|$)', 'avoid'),
            (r'omit\s+([\w\s,]+?)(?:\.|,|;|$)', 'omit'),
        ]
    
    def extract_negations(self, query: str) -> Dict[str, Set[str]]:
        """
        Extract negated entities from query
        Returns categorized negations
        """
        q_lower = query.lower()
        negations = {
            'countries': set(),
            'regions': set(),
            'products': set(),
            'topics': set(),
            'client_types': set()
        }
        
        for pattern, neg_type in self.negative_patterns:
            matches = re.findall(pattern, q_lower)
            for match in matches:
                # Parse the matched string for entities
                entities = self._parse_entities(match)
                for category, items in entities.items():
                    negations[category].update(items)
        
        return {k: list(v) for k, v in negations.items() if v}
    
    def _parse_entities(self, text: str) -> Dict[str, Set[str]]:
        """Parse text for known entities"""
        entities = {
            'countries': set(),
            'regions': set(),
            'products': set(),
            'topics': set(),
            'client_types': set()
        }
        
        text_upper = text.upper().strip()
        
        # Check for countries
        for alias, proper_name in alias_mapping.items():
            if re.search(rf"\b{re.escape(alias)}\b", text_upper):
                entities['countries'].add(proper_name)
        
        # Check for regions
        regions = ['ASIA', 'EUROPE', 'AMERICAS', 'AFRICA', 'AME', 'EMEA', 'APAC']
        for region in regions:
            if re.search(rf"\b{re.escape(region)}\b", text_upper):
                entities['regions'].add(region)
        
        # Check for products
        products = ['CASH', 'TRADE', 'FX', 'SECURITIES', 'TREASURY']
        for product in products:
            if re.search(rf"\b{re.escape(product)}\b", text_upper):
                entities['products'].add(product)
        
        # Check for topics
        topics = ['S2B', 'ACH', 'RTGS', 'SWIFT', 'WIRE', 'SEPA']
        for topic in topics:
            if re.search(rf"\b{re.escape(topic)}\b", text_upper):
                entities['topics'].add(topic)
        
        # Check for client types
        client_types = ['CORPORATES', 'FI', 'RETAIL', 'INSTITUTIONAL']
        for client_type in client_types:
            if re.search(rf"\b{re.escape(client_type)}\b", text_upper):
                entities['client_types'].add(client_type)
        
        return entities

# ========================================
# FILTER ADJUSTMENT LOGIC
# ========================================

def adjust_filters_for_negations(
    extracted_filters: Dict[str, List],
    negations: Dict[str, List],
    verbose: bool = False
) -> Tuple[Dict, Dict]:
    """
    Adjust filters based on negations
    Only removes contradicting values, doesn't add new ones
    """
    adjusted_filters = {}
    removed_items = {}
    
    for key, values in extracted_filters.items():
        if not values:
            continue
        
        # Map filter keys to negation keys
        negation_key_map = {
            'country': 'countries',
            'region': 'regions',
            'product': 'products',
            'topic': 'topics',
            'client_type': 'client_types'
        }
        
        neg_key = negation_key_map.get(key)
        if neg_key and neg_key in negations and negations[neg_key]:
            # Check for contradictions
            filter_values_set = set(values)
            negated_values_set = set(negations[neg_key])
            
            # Find values that are both in filter and negated
            contradictions = filter_values_set.intersection(negated_values_set)
            
            if contradictions:
                # Remove contradicting values
                remaining_values = list(filter_values_set - contradictions)
                if remaining_values:
                    adjusted_filters[key] = remaining_values
                # Track what was removed
                removed_items[key] = list(contradictions)
                
                if verbose:
                    print(f"‚ö†Ô∏è Removed {key}={list(contradictions)} due to negation")
            else:
                # No contradictions, keep original
                adjusted_filters[key] = values
        else:
            # No negations for this key, keep original
            adjusted_filters[key] = values
    
    return adjusted_filters, removed_items

# ========================================
# SEMANTIC SEARCH WITH PENALTIES
# ========================================

def semantic_search_with_penalties(
    table,
    model,
    query_text: str,
    filters: Dict[str, List],
    negations: Dict[str, List],
    limit: int = 20,
    semantic_threshold: float = 0.7,
    verbose: bool = False
) -> Tuple[pd.DataFrame, int]:
    """
    Perform semantic search with filter boosts and negation penalties
    """
    
    # Encode query
    qvec = model.encode(query_text)
    qvec = qvec / np.linalg.norm(qvec)
    
    # Stage 1: Get candidates (more than needed for re-ranking)
    candidate_limit = min(limit * 5, 100)
    
    # Build filter string for initial retrieval (optional)
    from lancedb_advanced_progressive_filter import build_filter_string
    filter_string = build_filter_string(filters) if filters else None
    
    # Get candidates
    search_query = table.search(qvec).metric("cosine").limit(candidate_limit)
    if filter_string and filters:
        # Apply filters for initial retrieval
        search_query = search_query.where(filter_string)
    
    candidates_df = search_query.to_pandas()
    
    if len(candidates_df) == 0:
        return pd.DataFrame(), 0
    
    # Stage 2: Score adjustment
    candidates_df['semantic_score'] = 1 - candidates_df['_distance']
    candidates_df['filter_score'] = 1.0
    
    # Boost documents matching filters
    for key, values in filters.items():
        if values and key in candidates_df.columns:
            if isinstance(values, list):
                matches = candidates_df[key].isin(values)
            else:
                matches = candidates_df[key] == values
            
            candidates_df.loc[matches, 'filter_score'] *= 1.3  # Boost matches
            candidates_df.loc[~matches, 'filter_score'] *= 0.8  # Mild penalty for non-matches
    
    # Apply heavy penalties for negated items
    if 'countries' in negations:
        for country in negations['countries']:
            if 'country' in candidates_df.columns:
                mask = candidates_df['country'].str.upper() == country.upper()
                candidates_df.loc[mask, 'filter_score'] *= 0.1  # Heavy penalty
                if verbose and mask.any():
                    print(f"   Penalized {mask.sum()} documents with country={country}")
    
    if 'regions' in negations:
        for region in negations['regions']:
            if 'region' in candidates_df.columns:
                mask = candidates_df['region'].str.upper() == region.upper()
                candidates_df.loc[mask, 'filter_score'] *= 0.1
    
    if 'topics' in negations:
        for topic in negations['topics']:
            if 'topic' in candidates_df.columns:
                mask = candidates_df['topic'].str.upper() == topic.upper()
                candidates_df.loc[mask, 'filter_score'] *= 0.1
    
    # Combine scores (semantic priority)
    candidates_df['final_score'] = (
        candidates_df['semantic_score'] * semantic_threshold +
        candidates_df['filter_score'] * (1 - semantic_threshold)
    )
    
    # Sort by final score and limit
    results_df = candidates_df.nlargest(limit, 'final_score').reset_index(drop=True)
    
    return results_df, len(results_df)

# ========================================
# MAIN INTEGRATED SEARCH
# ========================================

def integrated_semantic_search(
    table,
    model,
    session_id: str,
    query_text: str,
    max_results: int = 20,
    semantic_threshold: float = 0.7,
    enable_progressive: bool = True,
    verbose: bool = True
) -> Dict:
    """
    Complete integrated search pipeline
    
    1. Extract metadata filters (respects them as final)
    2. Detect negations
    3. Adjust filters ONLY for contradictions
    4. Perform semantic search with appropriate scoring
    """
    
    start_time = time.time()
    analyzer = NegationAnalyzer()
    
    if verbose:
        print("\n" + "="*60)
        print("üîç INTEGRATED SEMANTIC SEARCH")
        print("="*60)
        print(f"Query: '{query_text}'")
    
    # Step 1: Extract metadata filters (these are FINAL, not to be augmented)
    extracted_filters, processed_query = process_question(session_id, query_text)
    
    if verbose:
        print(f"Extracted filters: {extracted_filters}")
    
    # Step 2: Detect negations in query
    negations = analyzer.extract_negations(query_text)
    
    if verbose and negations:
        print(f"Detected negations: {negations}")
    
    # Step 3: Adjust filters ONLY for contradictions
    adjusted_filters, removed_items = adjust_filters_for_negations(
        extracted_filters, negations, verbose
    )
    
    if verbose:
        if removed_items:
            print(f"Removed contradicting items: {removed_items}")
        print(f"Final filters to apply: {adjusted_filters}")
        print("-"*60)
    
    # Step 4: Perform search (with or without progressive relaxation)
    if enable_progressive:
        results = progressive_search_with_penalties(
            table, model, query_text, adjusted_filters, negations,
            max_results, semantic_threshold, verbose
        )
    else:
        # Single search without progressive relaxation
        results_df, count = semantic_search_with_penalties(
            table, model, query_text, adjusted_filters, negations,
            max_results, semantic_threshold, verbose
        )
        results = {
            'results': results_df,
            'applied_filters': adjusted_filters,
            'removed_filters': [],
            'iterations': 1
        }
    
    # Add metadata to results
    results['original_query'] = query_text
    results['extracted_filters'] = extracted_filters
    results['negations'] = negations
    results['removed_items'] = removed_items
    results['execution_time'] = time.time() - start_time
    
    return results

def progressive_search_with_penalties(
    table,
    model,
    query_text: str,
    initial_filters: Dict[str, List],
    negations: Dict[str, List],
    max_results: int = 20,
    semantic_threshold: float = 0.7,
    verbose: bool = True
) -> Dict:
    """
    Progressive filter relaxation with negation penalties
    """
    
    filter_priority = {
        "topic": 1,
        "product": 2, 
        "client_type": 3,
        "region": 4,
        "country": 5
    }
    
    active_filters = initial_filters.copy()
    removed_filters = []
    iterations = 0
    
    while True:
        iterations += 1
        
        # Search with current filters
        results_df, count = semantic_search_with_penalties(
            table, model, query_text, active_filters, negations,
            max_results, semantic_threshold, verbose=(verbose and iterations == 1)
        )
        
        if verbose:
            print(f"\nüìä Iteration {iterations}:")
            print(f"   Active filters: {active_filters if active_filters else 'None'}")
            print(f"   Results found: {count}")
        
        # Check if we have enough results
        if count >= max_results or not active_filters:
            if verbose:
                if count >= max_results:
                    print(f"‚úÖ Target reached with {count} results")
                else:
                    print(f"‚ö†Ô∏è No more filters to remove. Found {count} results")
            break
        
        # Remove lowest priority filter
        filters_by_priority = sorted(
            active_filters.items(),
            key=lambda x: filter_priority.get(x[0], 999)
        )
        
        if filters_by_priority:
            key_to_remove, value_removed = filters_by_priority[0]
            del active_filters[key_to_remove]
            removed_filters.append({'key': key_to_remove, 'value': value_removed})
            
            if verbose:
                print(f"   ‚Üí Removing filter: {key_to_remove}")
    
    return {
        'results': results_df,
        'applied_filters': active_filters,
        'removed_filters': removed_filters,
        'iterations': iterations
    }

# ========================================
# USAGE EXAMPLES
# ========================================

def main():
    """Test the corrected system"""
    
    # Setup test database
    db = lancedb.connect("./lancedb_corrected")
    
    documents = [
        {
            'text': 'ACH payment cut-off time for Bangladesh is 3:00 PM local time',
            'metadata': {
                'region': 'ASIA', 'country': 'BANGLADESH', 'topic': 'ACH',
                'product': 'CASH', 'client_type': 'CORPORATES'
            }
        },
        {
            'text': 'S2B payment services available in Singapore for corporate clients',
            'metadata': {
                'region': 'ASIA', 'country': 'SINGAPORE', 'topic': 'S2B',
                'product': 'CASH', 'client_type': 'CORPORATES'
            }
        },
        {
            'text': 'Cut-off times for Malaysia ACH payments are 4:00 PM',
            'metadata': {
                'region': 'ASIA', 'country': 'MALAYSIA', 'topic': 'ACH',
                'product': 'CASH', 'client_type': 'RETAIL'
            }
        },
        {
            'text': 'S2B implementation for all Asian countries except Bangladesh',
            'metadata': {
                'region': 'ASIA', 'country': 'ALL', 'topic': 'S2B',
                'product': 'CASH', 'client_type': 'CORPORATES'
            }
        }
    ]
    
    # Create table
    data = []
    for i, doc in enumerate(documents):
        vec = model.encode(doc['text'])
        vec = vec / np.linalg.norm(vec)
        record = {
            'id': i,
            'text': doc['text'],
            'vector': vec,
            **doc['metadata']
        }
        data.append(record)
    
    if 'documents' in db.table_names():
        db.drop_table('documents')
    
    table = db.create_table('documents', pd.DataFrame(data))
    print(f"‚úÖ Created table with {len(data)} documents\n")
    
    # Test 1: Specific country query (should only filter Bangladesh)
    print("="*60)
    print("TEST 1: Specific Country Query")
    print("="*60)
    
    session_id = "test1"
    reset_filter_dict(session_id)
    query1 = "What is the cut off time for Bangladesh?"
    
    result1 = integrated_semantic_search(
        table, model, session_id, query1,
        max_results=5,
        enable_progressive=False,
        verbose=True
    )
    
    print("\nüìÑ Results:")
    if 'results' in result1 and not result1['results'].empty:
        for idx, row in result1['results'].head(3).iterrows():
            print(f"  {idx+1}. {row['text']}")
            print(f"     Country: {row['country']}, Score: {row.get('final_score', 0):.3f}\n")
    
    # Test 2: Query with negation
    print("\n" + "="*60)
    print("TEST 2: Query with Negation")
    print("="*60)
    
    session_id = "test2"
    reset_filter_dict(session_id)
    query2 = "S2B payment capability without Bangladesh"
    
    result2 = integrated_semantic_search(
        table, model, session_id, query2,
        max_results=5,
        enable_progressive=True,
        verbose=True
    )
    
    print("\nüìÑ Results:")
    if 'results' in result2 and not result2['results'].empty:
        for idx, row in result2['results'].head(3).iterrows():
            print(f"  {idx+1}. {row['text']}")
            print(f"     Country: {row['country']}, Score: {row.get('final_score', 0):.3f}\n")

if __name__ == "__main__":
    main()

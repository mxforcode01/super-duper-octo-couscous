from typing import List, Dict, Any

def to_context_items(reranked: List[Dict[str, Any]], max_chars: int = 1200) -> List[Dict[str, Any]]:
    """
    Normalize reranked Qdrant results into a minimal schema for LLM context.
    Expected format per item:
      {
        "id": 1234,
        "text": "...",
        "payload": {
            "region": "asia",
            "country": "korea",
            "filename": "client a rfp",
            "updated_date": "2024-12-31",
            "topic": "virtual account",  # ignored
        }
      }
    """
    items = []
    for i, d in enumerate(reranked, start=1):
        payload = d.get("payload", {}) or {}
        text = d.get("text", "").strip()
        if len(text) > max_chars:
            text = text[:max_chars].rstrip() + "…"
        items.append({
            "id": f"S{i}",
            "text": text,
            "region": payload.get("region", ""),
            "country": payload.get("country", ""),
            "filename": payload.get("filename", ""),
            "updated_date": payload.get("updated_date", ""),
        })
    return items

def render_context_pack(items: List[Dict[str, Any]]) -> str:
    """
    Render context into a compact LLM-friendly format.
    """
    lines = ["[CONTEXT]"]
    for it in items:
        # Build metadata string
        meta_bits = []
        if it["region"]: meta_bits.append(f"region={it['region']}")
        if it["country"]: meta_bits.append(f"country={it['country']}")
        if it["filename"]: meta_bits.append(f"file={it['filename']}")
        if it["updated_date"]: meta_bits.append(f"updated={it['updated_date']}")
        meta = "; ".join(meta_bits)

        header = f"--- {it['id']} ({meta})"
        body = it["text"]

        lines.append(header)
        lines.append(body)
        lines.append("")  # blank line
    lines.append("[/CONTEXT]")
    return "\n".join(lines)





   
@torch.inference_mode()
    def rerank(
        self,
        query: str,
        candidates: List[Candidate],
        top_k: int = 10,
        batch_size: int = 32,
        text_key: str = "text",
    ) -> List[Dict[str, Any]]:
        """
        Returns a list of candidates (dicts) sorted by cross-encoder score desc.
        Adds:
          - 'rerank_score': float
          - preserves any original fields (e.g., Qdrant id, payload, vec score)
        If input items are strings, they are returned as dicts with 'text'.
        """
        if not candidates:
            return []

        # Normalize candidates -> list of dicts with a 'text' field
        norm: List[Dict[str, Any]] = []
        for i, c in enumerate(candidates):
            if isinstance(c, str):
                norm.append({"idx": i, "text": c})
            elif isinstance(c, dict):
                if text_key not in c:
                    raise KeyError(f"Candidate dict at index {i} missing '{text_key}'")
                item = dict(c)  # shallow copy
                item.setdefault("idx", i)
                norm.append(item)
            else:
                raise TypeError(f"Unsupported candidate type at index {i}: {type(c)}")

        # Build (query, passage) pairs for scoring
        pairs = [(query, item[text_key]) for item in norm]

        # Batched scoring
        scores: List[float] = []
        for i in range(0, len(pairs), batch_size):
            batch_pairs = pairs[i : i + batch_size]
            batch_scores = self.model.predict(batch_pairs, convert_to_numpy=True)
            scores.extend(batch_scores.tolist())

        # Attach scores and sort
        for item, s in zip(norm, scores):
            item["rerank_score"] = float(s)

        norm.sort(key=lambda d: d["rerank_score"], reverse=True)
        return norm[: min(top_k, len(norm))]





















from math import ceil
from qdrant_client import models

def diversified_by_geo(
    client,
    collection_name: str,
    query_vec,
    countries: list[str] | None = None,
    regions: list[str] | None = None,
    *,
    final_k: int = 10,
    pool_mult: int = 2,
    single_country_pool: int = 20,
    per_country_min: int = 1,
    per_country_max: int = 6,
    respect_user_country_order: bool = True,
    region_to_countries: dict[str, list[str]] | None = None,
    additional_must: list[models.Condition] | None = None,
):
    countries = [c.strip() for c in (countries or []) if c and str(c).strip()]
    regions   = [r.strip() for r in (regions or []) if r and str(r).strip()]

    # (Optional) expand regions to countries to estimate how many countries might match
    expanded_countries = set(countries)
    if region_to_countries and regions:
        for r in regions:
            expanded_countries.update(region_to_countries.get(r, []))

    # Build filter: (country IN countries) OR (region IN regions)
    shoulds = []
    if countries:
        shoulds.append(models.FieldCondition(key="country", match=models.MatchAny(any=countries)))
    if regions:
        shoulds.append(models.FieldCondition(key="region", match=models.MatchAny(any=regions)))

    if shoulds:
        base_filter = models.Filter(must=(additional_must or None), should=shoulds)
    else:
        # No geo constraints requested → just pass through any additional must conditions
        base_filter = models.Filter(must=(additional_must or None))

    pool_k = max(final_k * pool_mult, final_k)

    # Estimate #countries to pick a sensible per-country cap
    if expanded_countries:
        n_countries = len(expanded_countries)
    else:
        # If we don't know, assume at least 1 country could appear
        n_countries = max(1, len(countries) or len(regions) or 1)

    if n_countries == 1:
        group_size = min(single_country_pool, per_country_max or single_country_pool)
    else:
        group_size = max(per_country_min, min(per_country_max, ceil(pool_k / n_countries)))

    # --- Grouped search by 'country'
    try:
        grouped = client.search_groups(
            collection_name=collection_name,
            query_vector=query_vec,
            query_filter=base_filter,
            group_by="country",
            group_size=group_size,
            limit=1024,                 # upper bound on number of groups returned
            with_payload=True,
            with_vectors=False,
        )
    except Exception:
        grouped = None

    # Safely read group identifier (works across client versions)
    buckets = {}
    if grouped and getattr(grouped, "groups", None):
        for g in grouped.groups:
            key = getattr(g, "group_id", getattr(g, "id", None))
            if key is None:
                continue
            buckets[key] = list(g.hits)

    # If no groups came back (e.g., field missing/index not created, or empty result),
    # fall back to a single ungrouped search.
    if not buckets:
        fill = client.search(
            collection_name=collection_name,
            query_vector=query_vec,
            query_filter=base_filter,
            limit=pool_k * 2,
            with_payload=True,
            with_vectors=False,
        )
        fill = sorted(fill, key=lambda h: h.score, reverse=True)[:pool_k]
        return fill[:final_k]

    # Decide round-robin order
    if respect_user_country_order and countries:
        order = [c for c in countries if c in buckets] + [c for c in buckets.keys() if c not in countries]
    else:
        order = list(buckets.keys())

    # Round-robin shortlist up to pool_k
    shortlist = []
    while len(shortlist) < pool_k and any(buckets[c] for c in order):
        for c in order:
            if buckets.get(c):
                shortlist.append(buckets[c].pop(0))
                if len(shortlist) >= pool_k:
                    break

    # If underfilled, do a global fill (ungrouped) with same filter
    if len(shortlist) < pool_k:
        fill = client.search(
            collection_name=collection_name,
            query_vector=query_vec,
            query_filter=base_filter,
            limit=pool_k * 2,
            with_payload=True,
            with_vectors=False,
        )
        seen = {h.id for h in shortlist}
        for h in fill:
            if h.id not in seen:
                shortlist.append(h)
                seen.add(h.id)
            if len(shortlist) >= pool_k:
                break

    # Final global sort + cap
    shortlist.sort(key=lambda h: h.score, reverse=True)
    return shortlist[:final_k]













def diversified_by_geo_with_fallback(
    client,
    collection_name,
    query_vec,
    countries=None,
    regions=None,
    *,
    final_k=10,
    pool_mult=2,
    pool_target=20,  # how many total you want to gather before trimming
    **kwargs
):
    # Step 1: normal diversified search
    shortlist = diversified_by_geo(
        client=client,
        collection_name=collection_name,
        query_vec=query_vec,
        countries=countries,
        regions=regions,
        final_k=final_k,
        pool_mult=pool_mult,
        **kwargs
    )

    # Step 2: if not enough candidates, relax geo filters
    if len(shortlist) < pool_target:
        fill = client.search(
            collection_name=collection_name,
            query_vector=query_vec,
            limit=pool_target * 2,  # fetch extra for safety
            with_payload=True,
            with_vectors=False,
        )
        seen = {h.id for h in shortlist}
        for h in fill:
            if h.id not in seen:
                shortlist.append(h)
                seen.add(h.id)
            if len(shortlist) >= pool_target:
                break

    # Step 3: global re-rank and trim
    shortlist.sort(key=lambda h: h.score, reverse=True)
    return shortlist[:final_k]


from math import ceil
from qdrant_client import models

def diversified_by_geo(
    client,
    collection_name: str,
    query_vec,
    countries: list[str] | None = None,
    regions: list[str] | None = None,
    *,
    final_k: int = 10,              # hard limit for your model
    pool_mult: int = 2,             # over-fetch for better selection (e.g., reranker)
    single_country_pool: int = 20,  # when only 1 country ultimately matches
    per_country_min: int = 1,
    per_country_max: int = 6,
    respect_user_country_order: bool = True,
    # Optional: if you can expand regions to countries locally, pass a mapping:
    region_to_countries: dict[str, list[str]] | None = None,
    additional_must: list[models.Condition] | None = None,  # e.g., product/status constraints
):
    """
    Returns up to `final_k` items diversified by country.
    Supports country and/or region filters. If both provided, union them.
    """
    countries = [c.strip() for c in (countries or []) if c and str(c).strip()]
    regions   = [r.strip() for r in (regions or []) if r and str(r).strip()]

    # If you know how to expand regions locally, do it to get better adaptive caps:
    expanded_countries = set(countries)
    if region_to_countries and regions:
        for r in regions:
            expanded_countries.update(region_to_countries.get(r, []))

    # Build the filter:
    # (country IN countries) OR (region IN regions)
    shoulds = []
    if countries:
        shoulds.append(models.FieldCondition(
            key="country", match=models.MatchAny(any=countries)
        ))
    if regions:
        shoulds.append(models.FieldCondition(
            key="region", match=models.MatchAny(any=regions)
        ))

    # If nothing specified, we don't constrain by geo (caller should decide if that's allowed)
    if not shoulds:
        base_filter = models.Filter(must=(additional_must or None))
    else:
        base_filter = models.Filter(
            must=(additional_must or None),
            should=shoulds,
        )

    # Candidate pool size and adaptive per-country cap
    pool_k = max(final_k * pool_mult, final_k)

    # Decide how many countries are *expected*.
    # If we could expand regions, use that; else we’ll let Qdrant figure it out.
    if expanded_countries:
        n_countries = len(expanded_countries)
    else:
        # We don't know the exact # countries that will match;
        # assume at least 1; group_size will be corrected by round-robin later.
        n_countries = max(1, len(countries) or len(regions) or 1)

    if n_countries == 1:
        group_size = min(single_country_pool, per_country_max or single_country_pool)
    else:
        target_per_country = ceil(pool_k / n_countries)
        group_size = max(per_country_min, min(per_country_max, target_per_country))

    # Use grouped search so each country contributes up to `group_size`
    res = client.search_groups(
        collection_name=collection_name,
        query_vector=query_vec,
        query_filter=base_filter,
        group_by="country",
        group_size=group_size,
        limit=1024,                 # upper bound on #groups; Qdrant returns only those with hits
        with_payload=True,
        with_vectors=False,
    )

    # Bucket per country
    buckets = {g.group_id: list(g.hits) for g in res.groups}

    # Determine round-robin order:
    # 1) explicit countries first (in user order) if present,
    # 2) then any other countries that matched via regions (stable order)
    if respect_user_country_order and countries:
        ordered = [c for c in countries if c in buckets] + [c for c in buckets.keys() if c not in countries]
    else:
        ordered = list(buckets.keys())

    # Build diversified shortlist (round-robin) up to pool_k
    shortlist = []
    while len(shortlist) < pool_k and any(buckets[c] for c in ordered):
        for c in ordered:
            if buckets.get(c):
                shortlist.append(buckets[c].pop(0))
                if len(shortlist) >= pool_k:
                    break

    # Fallback: if underfilled, do one ungrouped fill from the same filter
    if len(shortlist) < pool_k:
        fill = client.search(
            collection_name=collection_name,
            query_vector=query_vec,
            query_filter=base_filter,
            limit=pool_k * 2,
            with_payload=True,
            with_vectors=False,
        )
        seen = {h.id for h in shortlist}
        for h in fill:
            if h.id not in seen:
                shortlist.append(h)
                seen.add(h.id)
            if len(shortlist) >= pool_k:
                break

    # Global sort by score then trim to final_k (ready for reranker/LLM)
    shortlist.sort(key=lambda h: h.score, reverse=True)
    return shortlist[:final_k]


results = diversified_by_geo_with_fallback(
    client,
    collection_name="my_docs_bge",
    query_vec=query_vec,
    countries=["Singapore","Malaysia"],
    regions=["Asia"],
    final_k=10,
    pool_target=20,
    per_country_max=5
)

for r in results:
    print(r.id, r.score, r.payload.get("country"), r.payload.get("region"))
